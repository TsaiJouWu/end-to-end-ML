{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 **Get the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 **Download the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 **Load the data using pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 讀取 csv 檔案\n",
    "def loading_housing_data(housing_path = HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loading_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讀取整筆 datasets 數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讀取 datasets 前五項數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讀取 datasets 中每個欄位、count數量、資料型別 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "顯示 DataFrame 中「數值型態」變數的描述統計資訊，結果包括 count、平均值、標準差、最小值、25%，50%（中位數）、75% 和最大值。\n",
    "1. count：顯示每個特徵「非缺失值」的數量\n",
    "2. mean：顯示每個特徵的平均值\n",
    "3. standaerd deviation：可以衡量每個數據點與平均值的分散程度（標準差越大，數據越分散）\n",
    "4. minimum value：每個特徵的最小值\n",
    "5. 25%、50%、75%（Percentiles）：用來評估數據的分佈\n",
    "6. maximum value：每個特徵的最大值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 繪製直方圖(histgram)\n",
    "數據的範圍會被分為若干個區間，稱為「箱子」(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data.hist(bins= 50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 **Create testing datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **分割資料集**\n",
    "1. pick some instances randomly, typiclly 20% of the datasets\n",
    "\n",
    "2. dataset -> traing set and testing set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indicates = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "\n",
    "    test_indicates = shuffled_indicates[:test_set_size]\n",
    "    train_indicates = shuffled_indicates[test_set_size:]\n",
    "    return data.iloc[test_indicates], data.iloc[train_indicates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set, train_set = split_train_test(data, test_ratio=0.2)\n",
    "print(len(data))\n",
    "print(len(test_set))\n",
    "print(len(train_set))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Use CRC to calculate checksum of data**\n",
    "在原本的資料集中加入新的資料，新產生出的 testing set 中不會有已經訓練過的資料（避免資料窺探）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lambda id_: test_set_check(id_, test_ratio)\n",
    "➡︎ lambda 為匿名函數，它可以接受一個識別符 id_ ，然後將它傳遞給 test_set_check function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib import crc32\n",
    "\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    # Convert identifier to 64 bits integer, and perform bit operations\n",
    "    # 如果計算後的數值 < test ratio's 32 bit，則放入 testing set\n",
    "    return crc32(np.int64(identifier) & 0xffffffff) < test_ratio * 2**32\n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    # id column is unique identifier\n",
    "    ids = data[id_column] \n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    # 使用位反操作符 ~，將 in_test_set 中的布林值取反，可以得到在測試集之外的實例 (= traing set)\n",
    "    return data.loc[in_test_set], data.loc[~in_test_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the index (add index column)\n",
    "➡︎ the old index is added as a column, and a new sequential index is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = data.reset_index()\n",
    "test_set, train_set = split_train_test_by_id(data_id, test_ratio= 0.2, id_column= \"index\") \n",
    "test_set.head()\n",
    "\n",
    "data_id[\"id\"] = data[\"longitude\"] * 1000 + data[\"latitude\"]\n",
    "test_set, train_set = split_train_test_by_id(data_id, test_ratio=0.2, id_column= \"id\")\n",
    "\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn\n",
    "1. Scikit-Learn privide some function\n",
    "2. Alomost the same as split_train_set() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "當再次執行程式，會產生一個不同的 Test set，隨著時間的推移，ML 演算法將會看到整個資料集。\n",
    "\n",
    "1. 在第一次執行時儲存 Test set ，然後在後續執行中載入它。\n",
    "2. 呼叫 np.random.permutation() 之前設定隨機數生成器的種子（例如：np.random.seed(42)）⁠，確保每次拆分時都產生相同的隨機索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state= 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Stratified sampling 分層抽樣**\n",
    "假設「收入中位數」是預測房價中位數一個重要的屬性，我們希望 testing dataset 可以表示整個 dataset 裡面所包含的各種收入類別。\n",
    "\n",
    "由於「收入中位數」是一個連續數字，收入中位數基本上集中在 1.5 ~ 6 左右，利用 pd.cut() 將連續數值劃分為非連續區間並建立各區間所對應的label。\n",
    "\n",
    "您首先需要建立一個收入類別屬性，那就需要先回去觀察收入中位數直方圖分佈。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"income_cat\"] = pd.cut(data[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf], # np.inf -> infinite\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"income_cat\"].value_counts() # 計算\"income_cat\" 屬性中每個不同值的出現頻率\n",
    "data[\"income_cat\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset -> Traing set and Testing set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# 設定分割次數為 1 (n_splits=1)，測試集為資料集的 20% (test_size=0.2)，並設定隨機種子以確保結果可重複 (random_state=42)。\n",
    "split = StratifiedShuffleSplit(n_splits= 1, test_size= 0.2, random_state= 42)\n",
    "# 使用 split.split(data, data[\"income_cat\"]) 進行分層隨機分割，用 for 迴圈得到訓練集的索引 (train_index) 和測試集的索引 (test_index)。\n",
    "for train_index, test_index in split.split(data, data[\"income_cat\"]):\n",
    "    # 從原始資料集中得到相對應的訓練集和測試集資料\n",
    "    strat_train_set = data.loc[train_index] \n",
    "    strat_test_set = data.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"income_cat\"].value_counts() / len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 比較 testing set 中 income 類別\n",
    "觀察每個 Label 在 testing set 的分佈\n",
    "\n",
    "建立原始樣本、分層抽樣與純隨機抽樣的分佈表格，並評估分層抽樣和隨機抽樣方法對收入分類比例估算的準確性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算每個收入類別在資料集的比例，也就是用每個類別的樣本數去除以總樣本數。\n",
    "def income_cat_propotion(data):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "# 將資料集分為 train_set 和 test_set，其中 test_set 佔總體資料集的 20%。\n",
    "train_set, test_set = train_test_split(data, test_size= 0.2, random_state= 42)\n",
    "\n",
    "# 用 income_cat_propotion function 分別去計算整體資料集、分層抽樣測試集和隨機抽樣測試集的 income 分類比例\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall\" : income_cat_propotion(data),\n",
    "    \"Stratified\" : income_cat_propotion(strat_test_set),\n",
    "    \"Random\" : income_cat_propotion(test_set)\n",
    "}).sort_index()\n",
    "\n",
    "# 計算分層抽樣和隨機抽樣對整體資料集的 income 比例的誤差\n",
    "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100\n",
    "\n",
    "compare_props.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let data is back to its original state\n",
    "1. drop \"income_cat\" column\n",
    "2. axis=1 表示刪除列，inplace=True 表示在原地修改資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 **Discover and Visualize the Data to Insights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Copy the training set**\n",
    "\n",
    "- **避免修改原始數據**：在模型訓練的過程中，可能會對數據作修改、新增、刪除等動作，為了避免意外的數據修改，在進行任何操作之前，會對原始數據進行備份。\n",
    "\n",
    "- **保證訓練的一致性**：訓練模型時會反覆調整不同的模型或參數，為了確保每次實驗使用的數據是一致的，會對資料進行備份。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **使用 'matplotlib' 中的 'plot' 函數繪出房價、位置與人口的散佈圖，以加州的地理座標位置為基礎。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind = \"scatter\", x = \"longitude\", y = \"latitude\", alpha = 0.4, # alpha 設定點的透明度，可以觀察哪些地方重疊次數較多 (high-density areas)\n",
    "             s = housing[\"population\"] / 100, label = \"population\", figsize = (8,6),\n",
    "             c = \"median_house_value\", cmap = plt.get_cmap(\"jet\"), colorbar =True, # 使用 \"jet\" 顏色去映射，值小時為藍色，值大時為紅色。\n",
    "             sharex = False) # sharex = False 防止刻度共享，在有有多個子圖的情況下，它們的 x 軸將不會共用相同的刻度。\n",
    "plt.title(label = \"California housing prices\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the California image\n",
    "images_path = os.path.join(\"datasets\", \"images\", \"end_to_end_project\")\n",
    "os.makedirs(images_path, exist_ok=True)\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "filename = \"california.png\"\n",
    "print(\"Downloading\", filename)\n",
    "\n",
    "url = DOWNLOAD_ROOT + \"images/end_to_end_project/\" + filename\n",
    "urllib.request.urlretrieve(url, os.path.join(images_path, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **在地圖上用散佈圖，呈現不同區域的人口（= 點的大小）和房屋價格（= 點的顏色）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Read the California image\n",
    "california_img = mpimg.imread(os.path.join(images_path, filename))\n",
    "\n",
    "# 繪製房地產數據地圖\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# 資料散佈圖\n",
    "scatter = ax.scatter(x = housing['longitude'], y = housing['latitude'],\n",
    "                     s = housing['population']/100, c= housing['median_house_value'],\n",
    "                     cmap = plt.get_cmap(\"jet\"), alpha= 0.4, label = \"Population\")\n",
    "\n",
    "# 加入地圖背景\n",
    "ax.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n",
    "          cmap=plt.get_cmap(\"jet\"))\n",
    "\n",
    "# 設定標籤和標題\n",
    "ax.set_ylabel(\"Latitude\", fontsize= 14)\n",
    "ax.set_xlabel(\"Longitude\", fontsize= 14)\n",
    "ax.set_title(\"California Housing Prices\", fontsize=16)\n",
    "\n",
    "# 設定顏色條\n",
    "prices = housing[\"median_house_value\"]\n",
    "tick_values = np.linspace(prices.min(), prices.max(), 11)\n",
    "cbar = plt.colorbar(scatter, ticks=tick_values/prices.max())\n",
    "cbar.ax.set_yticklabels([\"$%dk\"%(round(v/1000)) for v in tick_values], fontsize= 10)\n",
    "cbar.set_label('Median House Value', fontsize=16)\n",
    "\n",
    "# 顯示圖例（在圖表中用來標示不同元素或類別的標籤）\n",
    "ax.legend(fontsize= 10)\n",
    "\n",
    "# 顯示圖表\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **房價與位置（海邊）和人口密度高度相關**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 Looking for correlations 尋找關聯性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()\n",
    "housing.drop(\"ocean_proximity\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Looking for relavance method 1：**\n",
    "1. Look at how much each attribute `correlates` with the \"median house value\".\n",
    "\n",
    "2.  使用 `corr()` 計算相關係數矩陣並與 \"median_house_value\"排序結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr() #計算 DataFrame 中的所有列的相關係數矩陣\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False) # ascending = False 代表降序排列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Looking for relavance method 2：**\n",
    "1. 取得與 \"median_house_value\" 前四個是正相關的屬性\n",
    "\n",
    "2. Use 'scatter_matrix' function 創建一個散佈矩陣\n",
    "\n",
    "3. 畫出資料集中多個屬性之間的散佈圖矩陣，此 matrix 可以 `視覺化` 資料中不同屬性之間的相互關係。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
    "plt.tight_layout() #自動調整子圖或軸的排版，呈現出最佳的圖形尺寸\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **比較 median_income 與 median_house_value 屬性間的相關程度**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median income versus median house value\n",
    "housing.plot(kind = \"scatter\", x = \"median_income\", y = \"median_house_value\", alpha = 0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Can be observed from the above figure：**\n",
    "1. The correlation is indeed very strong.\n",
    "\n",
    "2. We need to removing a few below ( 500k, 450k, 350k and 280k dollars). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **🚀 Experimenting with Attribute Combinations**\n",
    "\n",
    "Now you may want to do before preparing the data for Machine Learning algorithms is to try out various attribute combinations.\n",
    "\n",
    "1. **使用統計和視覺化工具**：去深入了解數據中各個特徵之間的關係\n",
    "\n",
    "2. **特徵的組合**：試圖嘗試組合各種不同的屬性，去創造更多有用的新特徵，提高 Model 的準確率和性能。\n",
    "\n",
    "3. **分析**：評估「新建立的特徵」與「目標變數」(\"median_house_value\") 的相關性，並計算「新建立的特徵」其價值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n",
    "\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **可以發現 \"rooms_per_household\" 的相關性比原始的特徵 (\"total_rooms\", \"total_bedrooms\") 高**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 **Prepare the Data for Machine Learning Algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 在訓練機器學習模型時，通常會 `把特徵和目標變量區分開來`。\n",
    "- housing 去掉了 \"median_house_value\" 目標變量的列，這樣 housing 中就只包含了特徵變量。\n",
    "\n",
    "- housing_labels 只包含了目標變量 \"median_house_value\" 的列的數據。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 **Data Cleaning 處理訓練集缺失資料**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 查看訓練集各行是否存在空值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_rows_idx = housing.isnull().any(axis=1)\n",
    "print(null_rows_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 查看訓練集含有空值的行數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_rows_idx.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.loc[null_rows_idx].head() # loc -> 根據行或列的標籤選擇數據"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 丟掉相應的區域：刪除有缺失值的行，整個區域的數據被刪除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_option1 = housing.copy()\n",
    "housing_option1.dropna(subset = [\"total_bedrooms\"], inplace=True) # Option 1\n",
    "housing_option1.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 丟掉整個屬性，會失去 \"total_bedrooms\" 這個特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_option2 = housing.copy()\n",
    "housing_option2.drop(\"total_bedrooms\", axis = 1, inplace=True) # Option 2\n",
    "housing_option2.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 利用特定的值（中位數）填充和替換缺失值，同時保留有缺失值的行並填上數值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_option3 = housing.copy()\n",
    "\n",
    "# 計算訓練集上的中位數\n",
    "median = housing[\"total_bedrooms\"].median()\n",
    "housing_option3[\"total_bedrooms\"].fillna(median, inplace = True) # Option 3\n",
    "\n",
    "housing_option3.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Imputer** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleImputer 的 `strategy 參數` 可以設置為不同的值，以指定對缺失值的填充策略：\n",
    "- **median**：使用特徵的平均值來填充缺失值\n",
    "\n",
    "- **mean**：使用特徵的均值來填充缺失值\n",
    "\n",
    "- **constant**：使用指定的常數值來填充缺失值\n",
    "\n",
    "    - 指定 fill_value 參數： SimpleImputer(strategy=\"constant\", fill_value=0)，這樣缺失值就會被填充為 0。\n",
    "\n",
    "- **most_frequent**：使用最常見的特徵值來填充缺失值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ fit() 與 transform() 方法的比較：\n",
    "\n",
    "- 前者只輸入數據不進行轉換，後者將輸入的數據進行轉換。\n",
    "\n",
    "- transform() 進行在 fit() 的基礎上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ 在 SimpleImputer 中使用 `fit_transform` 方法：\n",
    "- SimpleImputer 是一個轉換器\n",
    "\n",
    "- fit_transform 方法可以擬合數據和進行轉換\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 對數據進行缺失值的填充，確保模型不會因為缺失值而出現錯誤，並儲存 `每個特徵的中位數(Median)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "# 將每個屬性的缺失值替換為該屬性的中位數\n",
    "imputer = SimpleImputer(strategy= \"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 刪除 \"ocean_proximity\" 列，確保 housing_num 只包含數字資料\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis = 1)\n",
    "# 訓練集中的數值去擬和每個特徵的中位數\n",
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 每個特徵的平均值（不包含\"ocean_proximity\"）會儲存在 statistics_  array 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = imputer.transform(housing_num) # MatrixLike -> 涵蓋許多數據結構的變數（Numpy, DataFrame, spareMatrix...）\n",
    "# 將 x 結果放回 DataFrame 中\n",
    "housing_tr = pd.DataFrame(x, index=housing_num.index, columns= housing_num.columns)\n",
    "housing_tr[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 **Handling Text and Categorical Attributes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 先前都在處理數字屬性（float64），但在 housing 資料集中有一個**文字屬性（object）**-> \"ocean_proximity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "housing_cat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 **Numpy array**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"ocean_proximity\" 底下的值非任意文字，其可能的值有限，每個值代表一個類別(object)。\n",
    "\n",
    "> 使用Scikit-Learn的OrdinalEncoder類(Numpy array)，可以將此類別的「**文字轉換為數字**」，讓機器可以更好讀取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get category list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 **Sparse matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 可以觀察到 \"housing_cat_1hot\" 為一稀疏矩陣，而不是 Numpy 陣列：\n",
    "\n",
    "1. 稀疏矩陣： 稀疏矩陣使用一種壓縮的形式，僅儲存非零元素的值或位置，包括 COO（坐標）、CSR（壓縮行）、CSC（壓縮列）。\n",
    "\n",
    "2. Numpy 陣列： Numpy 陣列以密集（dense）的形式存儲，即所有元素都有固定的位置。\n",
    "\n",
    "➞ 總結來說，稀疏矩陣主要用於處理「數據中包含大量零值」的情況，以減少記憶體使用；Numpy 陣列則是一種通用的多維數組結構，對於「密集的數據」表示非常有效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ⭐️ **OneHotEncoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One-hot encoding 是一種將類別型變數轉換為二進制表示的方法，並且只有其中一個特徵的值為 1，其餘特徵的值均為 0。\n",
    "1. 每個類別變數的每個唯一值都將被映射到一個新的二進制特徵。\n",
    "\n",
    "2. 對於每個樣本，僅有屬於其原始類別的二進制特徵的值為 1，其餘特徵的值均為 0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 在 training set 使用 \"fit_transform()\" 方法，需要先「適應」OneHotEncoder 對 training data。\n",
    "1. 檢測類別變數的唯一值，為每個唯一值分配一個新的二進制特徵（0 or 1）。\n",
    "\n",
    "2. 將training data 轉換成新的二進制特徵\n",
    "\n",
    "3. 這樣模型就能夠理解這些特徵，OneHotEncoder 也學到了轉換規則。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_1hot.toarray() # Sparse matrix -> Numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "設定 sparse = False -> \"fit_transform()\"方法直接返回一個密集 NumPy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder_1 = OneHotEncoder(sparse_output = False)\n",
    "housing_cat_1hot = cat_encoder_1.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use OneHotEncoder kit to list categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder_1.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Pandas has a function called \"get_dummies()\" \n",
    "1. 數據包含類別變數（顏色、天氣、地區等）且變數為「字串形式」，機器學習模型可能需要「數值形式」的 input。\n",
    "\n",
    "2. 每個類別可以成為新的二進制特徵列，其值為 0 或 1。\n",
    "\n",
    "3. 將 \"ocean_proximity\" 列中的類別變數轉換以二進制表示，將結果合併回原始 DataFrame。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame({\"ocean_proximity\": [\"INLAND\", \"NEAR BAY\", 'NEAR OCEAN']})\n",
    "result = pd.get_dummies(df_test) # 使用 get_dummies() 將類別變數進行 one-hot 編碼\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 在測試集或其他新數據上使用 \"transform()\"，表示使用之前已經「適應」過的 OneHotEncoder 的轉換規則來對新資料進行轉換。\n",
    "1. fit_transform() 用於學習和轉換訓練數據\n",
    "2. transform() 是使用之前學到的轉換規則來轉換新的數據（可以應用在未知數據）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder_1.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed get_dummies() a DataFrame containing an unknown category (e.g., \"<2H OCEAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_unknown = pd.DataFrame({\"ocean_proximity\": [\"<2H OCEAN\", \"ISLAND\"]})\n",
    "pd.get_dummies(df_test_unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "處理未知類別 \"<2H OCEAN\"，OneHotEncoder 能夠檢測未知類別並提出異常。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder_1.handle_unknown = \"ignore\" \n",
    "cat_encoder_1.transform(df_test_unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder_1.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder_1.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = pd.DataFrame(cat_encoder_1.transform(df_test_unknown),\n",
    "                         columns = cat_encoder_1.get_feature_names_out(),\n",
    "                         index = df_test_unknown.index)\n",
    "print(df_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 **Feature Scaling and Transformation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在處理數據時，有的時候數值特徵的大小差異很大，這可能會影響到模型的表現，因為模型可能會「**偏向**」關注值較大的特徵。\n",
    "- 與所有轉換一樣，重要的是將縮放器只適合訓練資料，而不是整個資料集（包括測試集）。\n",
    "\n",
    "- 使用 `特徵縮放`，目的是將所有特徵的數值 `調整到相同的尺度`：\n",
    "    1. **Min-max scaling**：將所有特徵的值縮放到一個指定的範圍內，通常在 0 到 1 之間。\n",
    "    2. **Standardization**：將特徵的值轉換為標準常態分佈(normal distribution)，使它們的平均值為 0，標準差為 1。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> fit() or fit_transform() only use for the `training set`.\n",
    "\n",
    "> If you have a **trained scaler**, you can use **it** to transform() any other set, including testing set, validation set and new data. \n",
    "\n",
    "➞ Trained scaler 指的是「已經」使用特定資料集進行適應(fit)的特徵縮器）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Min-max scaling (Normalization)**\n",
    "\n",
    "1. 對每個屬性，值會被移動並重新縮放，讓它們最終介於 0 到 1 之間。\n",
    "\n",
    "2. **透過減去最小值並除以最大值減去最小值**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Scikit-Learn provides a transformer called \"MinMaxScaler\"\n",
    "\n",
    "`It has a \"feature_range\" hyperparameter（超參數）`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scalar = MinMaxScaler(feature_range=(-1,1))\n",
    "housing_num_min_max_scaled = min_max_scalar.fit_transform(housing_num) # housing_num 是純數值訓練集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Standardization**\n",
    "1. **減去平均值**：將每個數值減去訓練集的平均值，這樣標準化後的平均值就會變成零。\n",
    "\n",
    "2. **除以標準差**：將結果除以訓練集的標準差，這樣標準化後的標準差就會變成 1。\n",
    "\n",
    "3. 標準化不會將值設立到特定範圍，這對一些 ML 演算法來說可能是一個問題（例如，神經網路通常期望輸入值從0到1不等）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**稀疏矩陣（Sparse Matrix）的處理**\n",
    "\n",
    "1. 稀疏矩陣中有很多零值，如果直接使用標準的特徵縮放方式可能會破壞矩陣的稀疏結構。\n",
    "\n",
    "2. `StandardScaler(with_mean = Fasle)`使用 StandardScalar 用於標準化，並將其超參數設為False。\n",
    "\n",
    "3. 這樣代表資料在進行除法時只使用標準差，而不會減掉平均值，可以保留稀疏矩陣中的零值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "housing_num_std_scaled = std_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Input Feature’s distribution（輸入特徵）**\n",
    "輸入特徵是用來訓練機器學習模型的數據的屬性\n",
    "\n",
    "當 input feature 數據分佈是 heavy tail or deviation from normal distribution 時：\n",
    "\n",
    "1.  Heavy tail 處理重尾分布\n",
    "\n",
    "2. Multimodal distribution 多峰分佈處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1. Heavy tail 處理重尾分布**\n",
    "\n",
    "大多數值都集中在一個小範圍內，可以先對特徵進行轉換，縮小重尾的影響。\n",
    "\n",
    "用平方根或將特徵提升到0到1之間的某個冪次方，或者對數(log)變換。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 將資料集特徵換算為平方根"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"population\"].apply(np.sqrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 幂律分佈換算為 log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"population\"].apply(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **分桶（Bucketizing）**\n",
    "\n",
    "將其分為大致相等大小的桶（區間），然後將每個特徵值替換為其所屬`桶的index`。\n",
    "\n",
    "這創建了一個`接近均勻分佈`的特徵，就不需要更進一步的縮放。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(8, 3), sharey=True)\n",
    "housing[\"population\"].hist(ax=axs[0], bins=50)\n",
    "housing[\"population\"].apply(np.log).hist(ax=axs[1], bins=50)\n",
    "axs[0].set_xlabel(\"Population\")\n",
    "axs[1].set_xlabel(\"Log of population\")\n",
    "axs[0].set_ylabel(\"Number of districts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2. Multimodal distribution 處理多峰分佈**\n",
    "1. **分桶（Bucketizing）**：將其分為大致相等大小的桶，然後把每個類別替換為其所屬 `桶的index` 而不是數值。使用 OneHotencoder\n",
    "\n",
    "2. **相似的類別特徵**：對於多峰分佈，還可以添加表示特定峰值相似性的特徵。使用 `徑向基函數（RBF）` 計算相似性。\n",
    "\n",
    "    ( 輸出值會隨著輸入值遠離固定點而呈指數級別遞減 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "# 測量屋齡中位數與 35 之間的相似性\n",
    "age_simil_35 = rbf_kernel(housing[[\"housing_median_age\"]], [[35]], gamma = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **捕捉房屋年齡中位數與特定年齡值之間的相似性**\n",
    "\n",
    "使用 `RBF kernel` 來計算特徵間的相似性。存在多峰分佈中的峰值，新的特徵在房屋年齡中位數為 35 時達到峰值，此圖可以了解該特定年齡範圍是否與較低價格相關聯。\n",
    "\n",
    "1. 較小的 gamma 值導致相似性的影響範圍變大。這意味著更遠的數據點對相似性的貢獻也比較大，Kernel 的形狀較為平緩。\n",
    "\n",
    "2. 較大的 gamma 值導致相似性的影響範圍變小。這意味著只有較近的數據點對相似性的貢獻較大，Kernel 的形狀較為尖峭。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = np.linspace(housing[\"housing_median_age\"].min(),\n",
    "                   housing[\"housing_median_age\"].max(),\n",
    "                   500).reshape(-1, 1)\n",
    "gamma1 = 0.1\n",
    "gamma2 = 0.03\n",
    "rbf1 = rbf_kernel(ages, [[35]], gamma = gamma1) # gamma 控制了 RBF kernel 的形狀\n",
    "rbf2 = rbf_kernel(ages, [[35]], gamma = gamma2)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel(\"Housing median age\")\n",
    "ax1.set_ylabel(\"Number of districts\")\n",
    "ax1.hist(housing[\"housing_median_age\"], bins = 50)\n",
    "\n",
    "ax2 = ax1.twinx()  # create a twin axis that shares the same x-axis\n",
    "color = \"blue\"\n",
    "ax2.plot(ages, rbf1, color=color, label=\"gamma = 0.10\")\n",
    "ax2.plot(ages, rbf2, color=color, label=\"gamma = 0.03\", linestyle=\"--\")\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_ylabel(\"Age similarity\", color=color)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Target Values（目標變數）**\n",
    "\n",
    "目標變數是在監督學習中，我們希望模型預測或分類的數值或標籤。\n",
    "\n",
    "當目標變數的分佈為 heavy tail or heavy tail or deviation from normal distribution 時：\n",
    "\n",
    "1. 直接轉換目標變數\n",
    "\n",
    "2. 使用 TransformedTargetRegressor 簡化轉換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 直接轉換目標變數**\n",
    "\n",
    "- 對目標變數的轉換和還原過程較為明確，能夠更靈活地處理。\n",
    "\n",
    "- 可以 `手動控制` 轉換的過程，根據需求調整轉換器的參數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 直接使用 StandardScaler 進行目標變數的縮放\n",
    "target_scaler = StandardScaler()\n",
    "scaled_labels = target_scaler.fit_transform(housing_labels.to_frame())\n",
    "\n",
    "# 使用線性回歸模型擬合(fit)縮放後的目標變數\n",
    "model = LinearRegression()\n",
    "model.fit(housing[[\"median_income\"]], scaled_labels)\n",
    "\n",
    "# 當需要進行預測時，模型會呼叫回歸模型的 predict() 方法\n",
    "some_new_data = housing[[\"median_income\"]].iloc[:5]  # pretend this is new data\n",
    "scaled_predictions = model.predict(some_new_data)\n",
    "\n",
    "# 使用 transformer 的 inverse_transform() 方法還原預測值的縮放\n",
    "predictions = target_scaler.inverse_transform(scaled_predictions)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 使用 TransformedTargetRegressor 簡化轉換**\n",
    "\n",
    "- 在預測階段可以方便的處理轉換前後的數據\n",
    "\n",
    "- 在模型擬合和預測過程中，自動處理目標變數的轉換和還原\n",
    "\n",
    "- TransformedTargetRegressor 會 **自動使用 transformer** 對目標變數進行縮放，然後 `使用縮放後的目標變數` 訓練回歸模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "# 給定兩個參數 '線性回歸模型' 和 '目標變數的 transformer' 結核並進行目標變數轉換\n",
    "model = TransformedTargetRegressor(LinearRegression(), transformer = StandardScaler())\n",
    "\n",
    "# 用原始目標變數進行模型擬合，它會「自動處理」目標變數的縮放。\n",
    "model.fit(housing[[\"median_income\"]], housing_labels)\n",
    "\n",
    "# 使用模型預測新數據\n",
    "predictions = model.predict(some_new_data)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 **Customer Transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a customer transformer ➡︎ 改善特徵的分佈，使其更趨近於 `常態分佈(normal distribution)`，因為模型處理常態分佈的表現較好。**\n",
    "\n",
    "Scikit-Learn 中提供許多有用的轉換器，在一些情況下，需要撰寫自己的 Transformer 來執行自定義的轉換、清理操作或結合特定屬性。\n",
    "\n",
    "⭐️ Transform contains 對數轉換、方根轉換 ➡︎ 使特徵更接近常態分佈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **使用 FunctionTransformer 創建不同的轉換器**\n",
    "從對數轉換到計算高斯 RBF 相似度，以及組合特徵的例子，可以在 Scikit-Learn Pipeline 中使用，以應用相同的轉換邏輯。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 對某個特徵（這裡是人口數）進行對數轉換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# 使用 NumPy 的對數函數和指數函數 create 一個對數轉換器\n",
    "log_transformer = FunctionTransformer(np.log, inverse_func = np.exp)\n",
    "log_pop = log_transformer.transform(housing[[\"population\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 創建高斯RBF相似度轉換器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算與之前相同的 RBF -> 建立測量屋齡中位數與 35 之間的相似性的 transformer\n",
    "rbf_transformer = FunctionTransformer(rbf_kernel, kw_args = dict(Y = [[35.]], gamma = 0.1))\n",
    "age_simil_35 = rbf_transformer.transform(housing[[\"housing_median_age\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 創建地理位置相似度轉換器\n",
    "\n",
    "- RBF Kernel 是非線性函數，是計算在某個固定點到其他點的距離上的有兩個相似度值。\n",
    "- 在一個距離上 **存在兩個相應的輸出值**，因此 `不存在單一的逆函數`，無法將輸出值準確還原為原始輸入。\n",
    "\n",
    "- 在計算相似度時，不區分輸入中的特徵。如果有一個多個特徵的陣列，它會使用 `歐氏距離（2D距離）`來計算和衡量相似度（精準度降低）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_coords = 37.7749, -122.41 # sf_coords 存放舊金山的經緯度\n",
    "# 使用rbf_kernel函數作為轉換函數, kw_args參數 -> 用來傳遞附加的參數給rbf_kernel函數，包含固定點的坐標 'Y'（即舊金山的坐標）、gamma值。\n",
    "sf_transformer = FunctionTransformer(rbf_kernel, kw_args = dict(Y = [sf_coords], gamma = 0.1))\n",
    "sf_simil = sf_transformer.transform(housing[[\"latitude\", \"longitude\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 創建一個可以計算輸入特徵的比例的轉換器\n",
    "\n",
    "- 目的是使用 Customer transformer 來創造新的特徵，此特徵是原始特徵之間的比例，可能可以讓模型更好地捕捉特徵之間的關係。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 lambda function 計算了輸入陣列中第一列和第二列之間的比例\n",
    "ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])\n",
    "# 將包含兩行兩列的陣列，應用在 ratio_transformer 這個轉換器上\n",
    "result = ratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Framework**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass  # no hyperparameters to tune\n",
    "    \n",
    "    def fit(self, X, y = None): # fit() 通常用於執行必要的初始化\n",
    "        return self  # always return self\n",
    "    \n",
    "    def transform(self, X): # transform() 執行對數轉換\n",
    "        return np.log1p(X)  # log-transform the input array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sklearn.utils.validation 軟體包包含幾個可用於驗證輸入的函式。\n",
    "\n",
    "- 在 fit() 方法中設定 n_features_in_，確保傳遞給 transform() 或 predict() 的資料具有這個數量的特徵。\n",
    "\n",
    "- 所有 estimators 在**傳遞** DataFrame 時，都應在 fit() 方法中設定 `'feature_names_in_'`。\n",
    "\n",
    "- 當 **transformer 可以逆轉**時，都應該提供 aget `'get_feature_names_out'` 方法，以及 `'aninverseinverse_transform'` 方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "\n",
    "class StandardScalerClone(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, with_mean=True):\n",
    "        self.with_mean = with_mean\n",
    "\n",
    "    def fit(self, X, y = None): # y is required even through we don't use it !!\n",
    "        X = check_array(X) # check x  is whether not infinte float values.\n",
    "        self.mean_ = X.mean(axis = 0)\n",
    "        self.scale_ = X.std(axis = 0)\n",
    "        self.n_features_in_ = X.shape[1] # every estimator stores this in fit()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        assert self.n_features_in_ == X.shape[1] # promise self.n_features_in_ and X equal to number of feature.\n",
    "        if self.with_mean:\n",
    "            X = X - self.mean_\n",
    "        return X / self.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Customer transformer can use other estimators**\n",
    "\n",
    "- 可以從 sklearn.utils.estimator_checks 中將 instance 傳給 check_estimator() 來檢查自定義 estimators 是否尊重Scikit-Learn的API。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        self.kmeans_ = KMeans(self.n_clusters, n_init=10, random_state=self.random_state)\n",
    "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
    "        return self  # always return self\n",
    "    \n",
    "    def  transform(self, X):\n",
    "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma= self.gamma) # 使用 RBF 計算每個樣本與 clusters 的相似程度\n",
    "    \n",
    "    def get_feature_names_out(self, names = None):\n",
    "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)] # 回傳每個特徵名稱，每個名稱都代表一個相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_simil = ClusterSimilarity(n_clusters= 10, gamma= 1., random_state= 42)\n",
    "similarities = cluster_simil.fit_transform(housing[[\"latitude\", \"longitude\"]], sample_weight = housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[:3].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_renamed = housing.rename(columns={\n",
    "    \"latitude\": \"Latitude\", \"longitude\": \"Longitude\",\n",
    "    \"population\": \"Population\",\n",
    "    \"median_house_value\": \"Median house value (ᴜsᴅ)\"})\n",
    "housing_renamed[\"Max cluster similarity\"] = similarities.max(axis = 1)\n",
    "\n",
    "housing_renamed.plot(kind = \"scatter\", x = \"Longitude\", y = \"Latitude\", grid = True,\n",
    "                     s = housing_renamed[\"Population\"] / 100, label = \"Population\",\n",
    "                     c=\"Max cluster similarity\",\n",
    "                     cmap=\"jet\", colorbar=True,\n",
    "                     legend=True, sharex=False, figsize=(8, 6))\n",
    "\n",
    "plt.plot(cluster_simil.kmeans_.cluster_centers_[:, 1],\n",
    "         cluster_simil.kmeans_.cluster_centers_[:, 0],\n",
    "         linestyle = \"\", color = \"black\", marker = \"X\", markersize = 15, label = \"Cluster centers\")\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 **Transformation Pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **A machine learning pipeline is used to help automate machine learning workflow.**\n",
    "\n",
    "- **ML pipelines are pipelines are iterative as every step is repeated to `continuously improve the accuracy` of the model and achieve a successful algorithm.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Simple operation of pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pipeline 每個步驟的 **轉換器**（transformer）都應具備 `fit_transform() 方法` \n",
    "    - 因為在 Pipeline 中的每個步驟，都需要進行一些資料轉換。\n",
    "\n",
    "- Pipeline 最後一個步驟可以是 `任何類型` 的估計器（也可是轉換器）\n",
    "    - 因為在一個Pipeline中，最後一個步驟可能是一個預測模型，或者其他不需要進行轉換的估計器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 利用 scikit-learn 的 Pipeline 建立 Pipeline()\n",
    "\n",
    "2. 建立名為 \"num_pipleline\" `處理數值型特徵`（填補缺失值，然後對這些特徵進行標準化）為包含兩個步驟的 Pipeline：\n",
    "\n",
    "    - **impute** 步驟：使用 SimpleImputer 這個轉換器（transformer），將缺失值使用中位數進行填充。\n",
    "    \n",
    "    - **standardize** 步驟：使用 StandardScaler 這個轉換器，對特徵進行標準化（均值為0、平方差為1的標準常態分佈）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"standardize\", StandardScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **如果不想為每個 step & transformers 命名，則使用 `make_pipeline()` 方法。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Scikit-Learn estimater 會呈現一互動式圖表，視覺化 Pipeline 流程。** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "num_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 調用 Pipeline 的 fit() 方法時，會按順序在所有轉換器上依次調用 fit_transform()。\n",
    "\n",
    "- 將每個調用的輸出作為參數，傳遞給下一個調用，直到傳到最終的估計器（estimator）。\n",
    "\n",
    "- 對於**最終的估計器**，它 `僅調用 fit() 方法`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num_prepared = num_pipeline.fit_transform(housing_num) # housing_num 是純數值訓練集\n",
    "housing_num_prepared[:2].round(2) #輸出前兩行，並四捨五入取到小數點後兩位"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **將 Pipeline 處理數值類型特徵數據轉換為 DataFrame**\n",
    "- **可視化分析**：DataFrame 可以更容易使用 matplotlib 或 seaborn 等函式庫繪製直方圖、散佈圖等圖表，觀察數據的分佈與關聯性。\n",
    "\n",
    "- **對比原始數據**：將處理後的數據轉換為 DataFrame 可以更容易得比對與原始數據的差異\n",
    "\n",
    "- **方便後續操作**：後面的模型訓練、特徵選擇等操作，通常需要使用 DataFrame 或類似的數據結構。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing_num_prepared = pd.DataFrame(housing_num_prepared, columns= num_pipeline.get_feature_names_out(), index= housing_num.index)\n",
    "\n",
    "df_housing_num_prepared.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **檢查、獲取和修改 Pipeline 的結構和參數**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **.steps：列出 Pipeline 中的步驟**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **回傳 Pipeline 中索引為 1 的步驟**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **回傳 Pipeline 中最後一個步驟「以前」的 SubPipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **.named_steps[\"\"]：根據名稱查詢 Pipeline 中的「指定步驟」並回傳**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline.named_steps[\"standardscaler\"] #因為是使用 'make_pipeline' 幫我們生成每個步驟的名稱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **.set_params()：動態調整 Pipeline 步驟中參數的方法(不需重新建立整個 Pipeline)**\n",
    "1. 設置步驟的超參數\n",
    "\n",
    "2. 使用雙底線 __ 來指定步驟的名稱和超參數的名稱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline.set_params(simpleimputer__strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **建立專案的 Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **使用 \"ColumnTransformer\" 轉換器**\n",
    "- 主要應對具有多種數值和類別特徵的數據集\n",
    "\n",
    "- 可以用於將不同的轉換，應用於數據的不同部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **根據指定的名稱分別把對應的轉換器應用到不同的特徵集上**\n",
    "1. 建立兩個陣列分別放入數值型和文字型特徵\n",
    "\n",
    "2. 利用已建立的處理數值型特徵之 Pipeline\n",
    "\n",
    "3. 建立一處理文字轉數值型特徵之 Pipeline\n",
    "\n",
    "4. 建立一自定義的轉換器 preprocessing 的 ColumnTransformer 物件，分別將數值特徵集應用於 num_pipeline，將分類特徵集應用於 cat_pipeline。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribute = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n",
    "cat_attribute = [\"ocean_proximity\"]\n",
    "\n",
    "cat_pipeline = make_pipeline(SimpleImputer(strategy=\"most_frequent\"), \n",
    "                             OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "      (\"num\", num_pipeline, num_attribute),\n",
    "    (\"cat\", cat_pipeline, cat_attribute),])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **使用 \"make_column_transformer\" 和 \"make_column_selector\"**\n",
    "- \"make_column_transformer\" 可以根據特徵的資料類型（數值型或類別型），指定不同的轉換步驟。\n",
    "\n",
    "- \"make_column_selector\" 是讓你能夠從資料類型中，輕鬆地從 DataFrame 中選擇想要的列。\n",
    "\n",
    "- 其中 \"dtype_include\" 和 \"dtype_exclude\" 允許根據數據類型來選擇或排除特徵列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "\n",
    "preprocessing = make_column_transformer(\n",
    "    (num_pipeline, make_column_selector(dtype_include=np.number)), # 當 DataFrame 中某一列的數據類型是數字（例如整數或浮點數），這列就會被選中！\n",
    "    (cat_pipeline, make_column_selector(dtype_include=object)), # 當 DataFrame 中某一列的數據類型是文字（例如字符串）或其他對象，這列就會被選中！\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **將原有 housing 訓練集進行預處理 Pipeline**：\n",
    "1. **數值型特徵 (num_attribute) 的處理**：使用 num_pipeline 進行數值型特徵的填充缺失值（使用中位數），並進行標準化（使用標準差標準化）。\n",
    "\n",
    "2. **類別型特徵 (cat_attribute) 的處理**：使用 cat_pipeline 進行類別型特徵的填充缺失值（使用眾數）和 One-Hot Encoding。\n",
    "\n",
    "3. ColumnTransformer 將這兩個 transformer 對應到符合的特徵列，並將整個資料集轉換成預處理後的形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared = preprocessing.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 將經過預處理的訓練集 housing_prepared 轉換為 DataFrame 格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared_fr = pd.DataFrame(housing_prepared, columns=preprocessing.get_feature_names_out(), index=housing.index)\n",
    "housing_prepared_fr.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared_fr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Review to build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_ratio(X):\n",
    "    return X[:, [0]] / X[:, [1]]\n",
    "\n",
    "def ratio_name(function_transformer, feature_names_in):\n",
    "    return [\"ratio\"]  # feature names out\n",
    "\n",
    "def ratio_pipeline():\n",
    "    return make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
    "        StandardScaler())\n",
    "\n",
    "log_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n",
    "    StandardScaler())\n",
    "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
    "default_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
    "                                     StandardScaler())\n",
    "preprocessing = ColumnTransformer([\n",
    "        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n",
    "        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n",
    "        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n",
    "        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n",
    "                               \"households\", \"median_income\"]),\n",
    "        (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n",
    "        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n",
    "    ],\n",
    "    remainder=default_num_pipeline)  # one column remaining: housing_median_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared = preprocessing.fit_transform(housing)\n",
    "housing_prepared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 **Select and Train a Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **🚀 Training and Evaluating on the Training Set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Linear Regression**\n",
    "\n",
    "在一堆數據中，希望 `找到一條直線`，這條直線能夠最好地去擬合這些數據 ➡ 這條直線的方程式就是線性回歸模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 建立線性回歸模型（之後可以利用此模型進行預測）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = make_pipeline(preprocessing, LinearRegression()) # 建立一個包含數據預處理的 Transformer 和線性回歸模型的 Pipeline\n",
    "linear_regression.fit(housing, housing_labels) # 將模型擬合到訓練數據上 -> housing= 包含特徵特徵的 DataFrame, housing_labels= 目標變量（房價中位數）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 線性回歸模型對前五個樣本的預測結果為："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = linear_regression.predict(housing)\n",
    "housing_predictions[:5].round(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compare against the actual values：\n",
    "\n",
    "- 可以看到，模型的預測與實際值有一些偏差\n",
    "- 通常會使用不同的指標（例如均方根誤差，R平方等）來評估模型的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_labels.iloc[:5].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 計算預測值和實際值之間的誤差比例：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果誤差比率為 0.1，表示預測值比實際值多了 10%；如果誤差比率為 -0.1，表示預測值比實際值少了 10%。\n",
    "error_ratios = housing_predictions[:5].round(-2) / housing_labels.iloc[:5].values - 1 \n",
    "print(\", \".join([f\"{100 * ratio:.1f}%\" for ratio in error_ratios])) # join 是把將字符串列表中的元素用指定的分隔符號連接起來"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 計算線性回歸模型的均方根誤差(RMSE)：\n",
    "\n",
    "- RMSE 是衡量模型預測誤差的常見指標\n",
    "- 是計算預測值和實際值之間的平方誤差的平均值，然後取平方根。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin_rmse = mean_squared_error(housing_labels, housing_predictions, squared= False)\n",
    "lin_rmse "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Decision Tree Regressor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "想像有一堆問題，每一個問題都是基於某個特徵的值。通過回答這些問題 ➡︎ 最終可以得到一個結論或預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_regression =  make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))\n",
    "tree_regression.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = tree_regression.predict(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_rmse = mean_squared_error(housing_labels, housing_predictions,\n",
    "                              squared=False)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **🚀 Better Evaluation Using Cross-Validation (交叉驗證)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 **Fine-Tune Your Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning 模型的目的是進一步調整 `已經訓練過` 的模型，讓它更好地去適應特定的任務或數據。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 **Grid Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**超參數 (hyperparameters)**\n",
    "1. 是在訓練模型之前需要「手動調整」的參數，它們不會從訓練數據中學習。\n",
    "2. 手動調整超參數的一種方法是逐一嘗試不同的值，直到找到「最佳組合」最大程度地提高模型的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**網格搜尋 (Grid Search)**：是一種自動化的超參數調整方法\n",
    "1. 它通過「指定」超參數的可能取值範圍，對這些值的 `所有組合` 進行搜索。\n",
    "2. 使用交叉驗證（cross-validation）來評估模型效能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 **Randomized Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**隨機搜索（Randomized Search）**：是一種超參數調整的方法\n",
    "1. 在超參數搜索空間中隨機選擇組合進行評估，而不是嘗試所有可能的組合。\n",
    "2. 適用於超參數搜索空間較大的情況\n",
    "3. 不需要事先指定所有可能的超參數組合，而是 `指定迭代次數`。\n",
    "4. 只評估固定數量的隨機組合，它通常比網格搜索更高效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Grid Search verse Randomized Search\n",
    "1. 對於 `連續超參數`（或是離散的但可能值很多的情況）：\n",
    "\n",
    "    隨機搜索在每次迭代中都會探索不同的值，而不僅僅是網格搜索列出的那幾個值。\n",
    "2. 有 `多個超參數` 需要探索時，每個超參數都有多個可能的值：\n",
    "\n",
    "    **網格搜索**可能需要大量訓練模型的次數；**隨機搜索**可以根據選擇的迭代次數運行\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 **Ensemble Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結合表現最好的多個模型來提升系統性能的方法，這組模型通常會 `比單個最佳模型更有效` ，特別是當個別模型出現不同類型錯誤的情況下。\n",
    "\n",
    "1. 隨機森林 Ramdom forests\n",
    "\n",
    "2. Train and Fine-tune a k-nearest -> create an ensemble model (CH7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 **Analyzing the Best Models and Their Errors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **RandomForestRegressor** can indicate the relative importance of each attribute for making accurate predictions：\n",
    "\n",
    "1. **獲取最佳模型**：從隨機搜索中獲取最佳模型，包括預處理步驟和實際的 RandomForestRegressor。\n",
    "\n",
    "2. **獲取特徵重要性**：使用最佳模型中的 RandomForestRegressor，獲取每個特徵的相對重要性分數。\n",
    "\n",
    "3. **分析特徵重要性**：將特徵重要性分數按降序排列，顯示每個特徵名稱和對應的重要性分數。\n",
    "\n",
    "4. **優化模型**：根據特徵重要性的結果，考慮是否刪除不太有用的特徵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 **Evaluate Your System on the Test Set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "調整完模型後，最後會使用 Testing set 對最終模型進行評估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **🚀 Launch, Monitor, and Maintain Your System**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data keeps evolving, you will **need to update your datasets and retrain your model** regularly. \n",
    "\n",
    "- 定期收集新資料並設定 Lable\n",
    "- 訓練模型並自動微調超參數，根據需求設定自動執行週期\n",
    "- 寫另一個程式碼，在更新的測試集上評估新模型和以前的模型\n",
    "       \n",
    "    1. 如果效能未下降，則將模型部署到產線中。\n",
    "    2. 如果效能確實下降，確實地調查下降原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the model’s input data quality**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure you keep backups of every model**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
