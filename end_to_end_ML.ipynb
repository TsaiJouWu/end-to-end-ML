{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– **Get the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ **Download the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ **Load the data using pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# è®€å– csv æª”æ¡ˆ\n",
    "def loading_housing_data(housing_path = HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loading_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®€å–æ•´ç­† datasets æ•¸æ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®€å– datasets å‰äº”é …æ•¸æ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®€å– datasets ä¸­æ¯å€‹æ¬„ä½ã€countæ•¸é‡ã€è³‡æ–™å‹åˆ¥ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é¡¯ç¤º DataFrame ä¸­ã€Œæ•¸å€¼å‹æ…‹ã€è®Šæ•¸çš„æè¿°çµ±è¨ˆè³‡è¨Šï¼ŒçµæœåŒ…æ‹¬ countã€å¹³å‡å€¼ã€æ¨™æº–å·®ã€æœ€å°å€¼ã€25%ï¼Œ50%ï¼ˆä¸­ä½æ•¸ï¼‰ã€75% å’Œæœ€å¤§å€¼ã€‚\n",
    "1. countï¼šé¡¯ç¤ºæ¯å€‹ç‰¹å¾µã€Œéç¼ºå¤±å€¼ã€çš„æ•¸é‡\n",
    "2. meanï¼šé¡¯ç¤ºæ¯å€‹ç‰¹å¾µçš„å¹³å‡å€¼\n",
    "3. standaerd deviationï¼šå¯ä»¥è¡¡é‡æ¯å€‹æ•¸æ“šé»èˆ‡å¹³å‡å€¼çš„åˆ†æ•£ç¨‹åº¦ï¼ˆæ¨™æº–å·®è¶Šå¤§ï¼Œæ•¸æ“šè¶Šåˆ†æ•£ï¼‰\n",
    "4. minimum valueï¼šæ¯å€‹ç‰¹å¾µçš„æœ€å°å€¼\n",
    "5. 25%ã€50%ã€75%ï¼ˆPercentilesï¼‰ï¼šç”¨ä¾†è©•ä¼°æ•¸æ“šçš„åˆ†ä½ˆ\n",
    "6. maximum valueï¼šæ¯å€‹ç‰¹å¾µçš„æœ€å¤§å€¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ç¹ªè£½ç›´æ–¹åœ–(histgram)\n",
    "æ•¸æ“šçš„ç¯„åœæœƒè¢«åˆ†ç‚ºè‹¥å¹²å€‹å€é–“ï¼Œç¨±ç‚ºã€Œç®±å­ã€(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data.hist(bins= 50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ **Create testing datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **åˆ†å‰²è³‡æ–™é›†**\n",
    "1. pick some instances randomly, typiclly 20% of the datasets\n",
    "\n",
    "2. dataset -> traing set and testing set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indicates = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "\n",
    "    test_indicates = shuffled_indicates[:test_set_size]\n",
    "    train_indicates = shuffled_indicates[test_set_size:]\n",
    "    return data.iloc[test_indicates], data.iloc[train_indicates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set, train_set = split_train_test(data, test_ratio=0.2)\n",
    "print(len(data))\n",
    "print(len(test_set))\n",
    "print(len(train_set))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Use CRC to calculate checksum of data**\n",
    "åœ¨åŸæœ¬çš„è³‡æ–™é›†ä¸­åŠ å…¥æ–°çš„è³‡æ–™ï¼Œæ–°ç”¢ç”Ÿå‡ºçš„ testing set ä¸­ä¸æœƒæœ‰å·²ç¶“è¨“ç·´éçš„è³‡æ–™ï¼ˆé¿å…è³‡æ–™çªºæ¢ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lambda id_: test_set_check(id_, test_ratio)\n",
    "â¡ï¸ lambda ç‚ºåŒ¿åå‡½æ•¸ï¼Œå®ƒå¯ä»¥æ¥å—ä¸€å€‹è­˜åˆ¥ç¬¦ id_ ï¼Œç„¶å¾Œå°‡å®ƒå‚³éçµ¦ test_set_check function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib import crc32\n",
    "\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    # Convert identifier to 64 bits integer, and perform bit operations\n",
    "    # å¦‚æœè¨ˆç®—å¾Œçš„æ•¸å€¼ < test ratio's 32 bitï¼Œå‰‡æ”¾å…¥ testing set\n",
    "    return crc32(np.int64(identifier) & 0xffffffff) < test_ratio * 2**32\n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    # id column is unique identifier\n",
    "    ids = data[id_column] \n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    # ä½¿ç”¨ä½åæ“ä½œç¬¦ ~ï¼Œå°‡ in_test_set ä¸­çš„å¸ƒæ—å€¼å–åï¼Œå¯ä»¥å¾—åˆ°åœ¨æ¸¬è©¦é›†ä¹‹å¤–çš„å¯¦ä¾‹ (= traing set)\n",
    "    return data.loc[in_test_set], data.loc[~in_test_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the index (add index column)\n",
    "â¡ï¸ the old index is added as a column, and a new sequential index is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = data.reset_index()\n",
    "test_set, train_set = split_train_test_by_id(data_id, test_ratio= 0.2, id_column= \"index\") \n",
    "test_set.head()\n",
    "\n",
    "data_id[\"id\"] = data[\"longitude\"] * 1000 + data[\"latitude\"]\n",
    "test_set, train_set = split_train_test_by_id(data_id, test_ratio=0.2, id_column= \"id\")\n",
    "\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn\n",
    "1. Scikit-Learn privide some function\n",
    "2. Alomost the same as split_train_set() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç•¶å†æ¬¡åŸ·è¡Œç¨‹å¼ï¼Œæœƒç”¢ç”Ÿä¸€å€‹ä¸åŒçš„ Test setï¼Œéš¨è‘—æ™‚é–“çš„æ¨ç§»ï¼ŒML æ¼”ç®—æ³•å°‡æœƒçœ‹åˆ°æ•´å€‹è³‡æ–™é›†ã€‚\n",
    "\n",
    "1. åœ¨ç¬¬ä¸€æ¬¡åŸ·è¡Œæ™‚å„²å­˜ Test set ï¼Œç„¶å¾Œåœ¨å¾ŒçºŒåŸ·è¡Œä¸­è¼‰å…¥å®ƒã€‚\n",
    "2. å‘¼å« np.random.permutation() ä¹‹å‰è¨­å®šéš¨æ©Ÿæ•¸ç”Ÿæˆå™¨çš„ç¨®å­ï¼ˆä¾‹å¦‚ï¼šnp.random.seed(42)ï¼‰â ï¼Œç¢ºä¿æ¯æ¬¡æ‹†åˆ†æ™‚éƒ½ç”¢ç”Ÿç›¸åŒçš„éš¨æ©Ÿç´¢å¼•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state= 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Stratified sampling åˆ†å±¤æŠ½æ¨£**\n",
    "å‡è¨­ã€Œæ”¶å…¥ä¸­ä½æ•¸ã€æ˜¯é æ¸¬æˆ¿åƒ¹ä¸­ä½æ•¸ä¸€å€‹é‡è¦çš„å±¬æ€§ï¼Œæˆ‘å€‘å¸Œæœ› testing dataset å¯ä»¥è¡¨ç¤ºæ•´å€‹ dataset è£¡é¢æ‰€åŒ…å«çš„å„ç¨®æ”¶å…¥é¡åˆ¥ã€‚\n",
    "\n",
    "ç”±æ–¼ã€Œæ”¶å…¥ä¸­ä½æ•¸ã€æ˜¯ä¸€å€‹é€£çºŒæ•¸å­—ï¼Œæ”¶å…¥ä¸­ä½æ•¸åŸºæœ¬ä¸Šé›†ä¸­åœ¨ 1.5 ~ 6 å·¦å³ï¼Œåˆ©ç”¨ pd.cut() å°‡é€£çºŒæ•¸å€¼åŠƒåˆ†ç‚ºéé€£çºŒå€é–“ä¸¦å»ºç«‹å„å€é–“æ‰€å°æ‡‰çš„labelã€‚\n",
    "\n",
    "æ‚¨é¦–å…ˆéœ€è¦å»ºç«‹ä¸€å€‹æ”¶å…¥é¡åˆ¥å±¬æ€§ï¼Œé‚£å°±éœ€è¦å…ˆå›å»è§€å¯Ÿæ”¶å…¥ä¸­ä½æ•¸ç›´æ–¹åœ–åˆ†ä½ˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"income_cat\"] = pd.cut(data[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf], # np.inf -> infinite\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"income_cat\"].value_counts() # è¨ˆç®—\"income_cat\" å±¬æ€§ä¸­æ¯å€‹ä¸åŒå€¼çš„å‡ºç¾é »ç‡\n",
    "data[\"income_cat\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset -> Traing set and Testing set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# è¨­å®šåˆ†å‰²æ¬¡æ•¸ç‚º 1 (n_splits=1)ï¼Œæ¸¬è©¦é›†ç‚ºè³‡æ–™é›†çš„ 20% (test_size=0.2)ï¼Œä¸¦è¨­å®šéš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿çµæœå¯é‡è¤‡ (random_state=42)ã€‚\n",
    "split = StratifiedShuffleSplit(n_splits= 1, test_size= 0.2, random_state= 42)\n",
    "# ä½¿ç”¨ split.split(data, data[\"income_cat\"]) é€²è¡Œåˆ†å±¤éš¨æ©Ÿåˆ†å‰²ï¼Œç”¨ for è¿´åœˆå¾—åˆ°è¨“ç·´é›†çš„ç´¢å¼• (train_index) å’Œæ¸¬è©¦é›†çš„ç´¢å¼• (test_index)ã€‚\n",
    "for train_index, test_index in split.split(data, data[\"income_cat\"]):\n",
    "    # å¾åŸå§‹è³‡æ–™é›†ä¸­å¾—åˆ°ç›¸å°æ‡‰çš„è¨“ç·´é›†å’Œæ¸¬è©¦é›†è³‡æ–™\n",
    "    strat_train_set = data.loc[train_index] \n",
    "    strat_test_set = data.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"income_cat\"].value_counts() / len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### æ¯”è¼ƒ testing set ä¸­ income é¡åˆ¥\n",
    "è§€å¯Ÿæ¯å€‹ Label åœ¨ testing set çš„åˆ†ä½ˆ\n",
    "\n",
    "å»ºç«‹åŸå§‹æ¨£æœ¬ã€åˆ†å±¤æŠ½æ¨£èˆ‡ç´”éš¨æ©ŸæŠ½æ¨£çš„åˆ†ä½ˆè¡¨æ ¼ï¼Œä¸¦è©•ä¼°åˆ†å±¤æŠ½æ¨£å’Œéš¨æ©ŸæŠ½æ¨£æ–¹æ³•å°æ”¶å…¥åˆ†é¡æ¯”ä¾‹ä¼°ç®—çš„æº–ç¢ºæ€§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨ˆç®—æ¯å€‹æ”¶å…¥é¡åˆ¥åœ¨è³‡æ–™é›†çš„æ¯”ä¾‹ï¼Œä¹Ÿå°±æ˜¯ç”¨æ¯å€‹é¡åˆ¥çš„æ¨£æœ¬æ•¸å»é™¤ä»¥ç¸½æ¨£æœ¬æ•¸ã€‚\n",
    "def income_cat_propotion(data):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "# å°‡è³‡æ–™é›†åˆ†ç‚º train_set å’Œ test_setï¼Œå…¶ä¸­ test_set ä½”ç¸½é«”è³‡æ–™é›†çš„ 20%ã€‚\n",
    "train_set, test_set = train_test_split(data, test_size= 0.2, random_state= 42)\n",
    "\n",
    "# ç”¨ income_cat_propotion function åˆ†åˆ¥å»è¨ˆç®—æ•´é«”è³‡æ–™é›†ã€åˆ†å±¤æŠ½æ¨£æ¸¬è©¦é›†å’Œéš¨æ©ŸæŠ½æ¨£æ¸¬è©¦é›†çš„ income åˆ†é¡æ¯”ä¾‹\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall\" : income_cat_propotion(data),\n",
    "    \"Stratified\" : income_cat_propotion(strat_test_set),\n",
    "    \"Random\" : income_cat_propotion(test_set)\n",
    "}).sort_index()\n",
    "\n",
    "# è¨ˆç®—åˆ†å±¤æŠ½æ¨£å’Œéš¨æ©ŸæŠ½æ¨£å°æ•´é«”è³‡æ–™é›†çš„ income æ¯”ä¾‹çš„èª¤å·®\n",
    "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100\n",
    "\n",
    "compare_props.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let data is back to its original state\n",
    "1. drop \"income_cat\" column\n",
    "2. axis=1 è¡¨ç¤ºåˆªé™¤åˆ—ï¼Œinplace=True è¡¨ç¤ºåœ¨åŸåœ°ä¿®æ”¹è³‡æ–™é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– **Discover and Visualize the Data to Insights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Copy the training set**\n",
    "\n",
    "- **é¿å…ä¿®æ”¹åŸå§‹æ•¸æ“š**ï¼šåœ¨æ¨¡å‹è¨“ç·´çš„éç¨‹ä¸­ï¼Œå¯èƒ½æœƒå°æ•¸æ“šä½œä¿®æ”¹ã€æ–°å¢ã€åˆªé™¤ç­‰å‹•ä½œï¼Œç‚ºäº†é¿å…æ„å¤–çš„æ•¸æ“šä¿®æ”¹ï¼Œåœ¨é€²è¡Œä»»ä½•æ“ä½œä¹‹å‰ï¼Œæœƒå°åŸå§‹æ•¸æ“šé€²è¡Œå‚™ä»½ã€‚\n",
    "\n",
    "- **ä¿è­‰è¨“ç·´çš„ä¸€è‡´æ€§**ï¼šè¨“ç·´æ¨¡å‹æ™‚æœƒåè¦†èª¿æ•´ä¸åŒçš„æ¨¡å‹æˆ–åƒæ•¸ï¼Œç‚ºäº†ç¢ºä¿æ¯æ¬¡å¯¦é©—ä½¿ç”¨çš„æ•¸æ“šæ˜¯ä¸€è‡´çš„ï¼Œæœƒå°è³‡æ–™é€²è¡Œå‚™ä»½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ä½¿ç”¨ 'matplotlib' ä¸­çš„ 'plot' å‡½æ•¸ç¹ªå‡ºæˆ¿åƒ¹ã€ä½ç½®èˆ‡äººå£çš„æ•£ä½ˆåœ–ï¼Œä»¥åŠ å·çš„åœ°ç†åº§æ¨™ä½ç½®ç‚ºåŸºç¤ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind = \"scatter\", x = \"longitude\", y = \"latitude\", alpha = 0.4, # alpha è¨­å®šé»çš„é€æ˜åº¦ï¼Œå¯ä»¥è§€å¯Ÿå“ªäº›åœ°æ–¹é‡ç–Šæ¬¡æ•¸è¼ƒå¤š (high-density areas)\n",
    "             s = housing[\"population\"] / 100, label = \"population\", figsize = (8,6),\n",
    "             c = \"median_house_value\", cmap = plt.get_cmap(\"jet\"), colorbar =True, # ä½¿ç”¨ \"jet\" é¡è‰²å»æ˜ å°„ï¼Œå€¼å°æ™‚ç‚ºè—è‰²ï¼Œå€¼å¤§æ™‚ç‚ºç´…è‰²ã€‚\n",
    "             sharex = False) # sharex = False é˜²æ­¢åˆ»åº¦å…±äº«ï¼Œåœ¨æœ‰æœ‰å¤šå€‹å­åœ–çš„æƒ…æ³ä¸‹ï¼Œå®ƒå€‘çš„ x è»¸å°‡ä¸æœƒå…±ç”¨ç›¸åŒçš„åˆ»åº¦ã€‚\n",
    "plt.title(label = \"California housing prices\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the California image\n",
    "images_path = os.path.join(\"datasets\", \"images\", \"end_to_end_project\")\n",
    "os.makedirs(images_path, exist_ok=True)\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "filename = \"california.png\"\n",
    "print(\"Downloading\", filename)\n",
    "\n",
    "url = DOWNLOAD_ROOT + \"images/end_to_end_project/\" + filename\n",
    "urllib.request.urlretrieve(url, os.path.join(images_path, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **åœ¨åœ°åœ–ä¸Šç”¨æ•£ä½ˆåœ–ï¼Œå‘ˆç¾ä¸åŒå€åŸŸçš„äººå£ï¼ˆ= é»çš„å¤§å°ï¼‰å’Œæˆ¿å±‹åƒ¹æ ¼ï¼ˆ= é»çš„é¡è‰²ï¼‰**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Read the California image\n",
    "california_img = mpimg.imread(os.path.join(images_path, filename))\n",
    "\n",
    "# ç¹ªè£½æˆ¿åœ°ç”¢æ•¸æ“šåœ°åœ–\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# è³‡æ–™æ•£ä½ˆåœ–\n",
    "scatter = ax.scatter(x = housing['longitude'], y = housing['latitude'],\n",
    "                     s = housing['population']/100, c= housing['median_house_value'],\n",
    "                     cmap = plt.get_cmap(\"jet\"), alpha= 0.4, label = \"Population\")\n",
    "\n",
    "# åŠ å…¥åœ°åœ–èƒŒæ™¯\n",
    "ax.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n",
    "          cmap=plt.get_cmap(\"jet\"))\n",
    "\n",
    "# è¨­å®šæ¨™ç±¤å’Œæ¨™é¡Œ\n",
    "ax.set_ylabel(\"Latitude\", fontsize= 14)\n",
    "ax.set_xlabel(\"Longitude\", fontsize= 14)\n",
    "ax.set_title(\"California Housing Prices\", fontsize=16)\n",
    "\n",
    "# è¨­å®šé¡è‰²æ¢\n",
    "prices = housing[\"median_house_value\"]\n",
    "tick_values = np.linspace(prices.min(), prices.max(), 11)\n",
    "cbar = plt.colorbar(scatter, ticks=tick_values/prices.max())\n",
    "cbar.ax.set_yticklabels([\"$%dk\"%(round(v/1000)) for v in tick_values], fontsize= 10)\n",
    "cbar.set_label('Median House Value', fontsize=16)\n",
    "\n",
    "# é¡¯ç¤ºåœ–ä¾‹ï¼ˆåœ¨åœ–è¡¨ä¸­ç”¨ä¾†æ¨™ç¤ºä¸åŒå…ƒç´ æˆ–é¡åˆ¥çš„æ¨™ç±¤ï¼‰\n",
    "ax.legend(fontsize= 10)\n",
    "\n",
    "# é¡¯ç¤ºåœ–è¡¨\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **æˆ¿åƒ¹èˆ‡ä½ç½®ï¼ˆæµ·é‚Šï¼‰å’Œäººå£å¯†åº¦é«˜åº¦ç›¸é—œ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ Looking for correlations å°‹æ‰¾é—œè¯æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()\n",
    "housing.drop(\"ocean_proximity\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Looking for relavance method 1ï¼š**\n",
    "1. Look at how much each attribute `correlates` with the \"median house value\".\n",
    "\n",
    "2.  ä½¿ç”¨ `corr()` è¨ˆç®—ç›¸é—œä¿‚æ•¸çŸ©é™£ä¸¦èˆ‡ \"median_house_value\"æ’åºçµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr() #è¨ˆç®— DataFrame ä¸­çš„æ‰€æœ‰åˆ—çš„ç›¸é—œä¿‚æ•¸çŸ©é™£\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False) # ascending = False ä»£è¡¨é™åºæ’åˆ—"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Looking for relavance method 2ï¼š**\n",
    "1. å–å¾—èˆ‡ \"median_house_value\" å‰å››å€‹æ˜¯æ­£ç›¸é—œçš„å±¬æ€§\n",
    "\n",
    "2. Use 'scatter_matrix' function å‰µå»ºä¸€å€‹æ•£ä½ˆçŸ©é™£\n",
    "\n",
    "3. ç•«å‡ºè³‡æ–™é›†ä¸­å¤šå€‹å±¬æ€§ä¹‹é–“çš„æ•£ä½ˆåœ–çŸ©é™£ï¼Œæ­¤ matrix å¯ä»¥ `è¦–è¦ºåŒ–` è³‡æ–™ä¸­ä¸åŒå±¬æ€§ä¹‹é–“çš„ç›¸äº’é—œä¿‚ã€‚ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
    "plt.tight_layout() #è‡ªå‹•èª¿æ•´å­åœ–æˆ–è»¸çš„æ’ç‰ˆï¼Œå‘ˆç¾å‡ºæœ€ä½³çš„åœ–å½¢å°ºå¯¸\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **æ¯”è¼ƒ median_income èˆ‡ median_house_value å±¬æ€§é–“çš„ç›¸é—œç¨‹åº¦**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median income versus median house value\n",
    "housing.plot(kind = \"scatter\", x = \"median_income\", y = \"median_house_value\", alpha = 0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Can be observed from the above figureï¼š**\n",
    "1. The correlation is indeed very strong.\n",
    "\n",
    "2. We need to removing a few below ( 500k, 450k, 350k and 280k dollars). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ğŸš€ Experimenting with Attribute Combinations**\n",
    "\n",
    "Now you may want to do before preparing the data for Machine Learning algorithms is to try out various attribute combinations.\n",
    "\n",
    "1. **ä½¿ç”¨çµ±è¨ˆå’Œè¦–è¦ºåŒ–å·¥å…·**ï¼šå»æ·±å…¥äº†è§£æ•¸æ“šä¸­å„å€‹ç‰¹å¾µä¹‹é–“çš„é—œä¿‚\n",
    "\n",
    "2. **ç‰¹å¾µçš„çµ„åˆ**ï¼šè©¦åœ–å˜—è©¦çµ„åˆå„ç¨®ä¸åŒçš„å±¬æ€§ï¼Œå»å‰µé€ æ›´å¤šæœ‰ç”¨çš„æ–°ç‰¹å¾µï¼Œæé«˜ Model çš„æº–ç¢ºç‡å’Œæ€§èƒ½ã€‚\n",
    "\n",
    "3. **åˆ†æ**ï¼šè©•ä¼°ã€Œæ–°å»ºç«‹çš„ç‰¹å¾µã€èˆ‡ã€Œç›®æ¨™è®Šæ•¸ã€(\"median_house_value\") çš„ç›¸é—œæ€§ï¼Œä¸¦è¨ˆç®—ã€Œæ–°å»ºç«‹çš„ç‰¹å¾µã€å…¶åƒ¹å€¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n",
    "\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **å¯ä»¥ç™¼ç¾ \"rooms_per_household\" çš„ç›¸é—œæ€§æ¯”åŸå§‹çš„ç‰¹å¾µ (\"total_rooms\", \"total_bedrooms\") é«˜**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– **Prepare the Data for Machine Learning Algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> åœ¨è¨“ç·´æ©Ÿå™¨å­¸ç¿’æ¨¡å‹æ™‚ï¼Œé€šå¸¸æœƒ `æŠŠç‰¹å¾µå’Œç›®æ¨™è®Šé‡å€åˆ†é–‹ä¾†`ã€‚\n",
    "- housing å»æ‰äº† \"median_house_value\" ç›®æ¨™è®Šé‡çš„åˆ—ï¼Œé€™æ¨£ housing ä¸­å°±åªåŒ…å«äº†ç‰¹å¾µè®Šé‡ã€‚\n",
    "\n",
    "- housing_labels åªåŒ…å«äº†ç›®æ¨™è®Šé‡ \"median_house_value\" çš„åˆ—çš„æ•¸æ“šã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ **Data Cleaning è™•ç†è¨“ç·´é›†ç¼ºå¤±è³‡æ–™**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> æŸ¥çœ‹è¨“ç·´é›†å„è¡Œæ˜¯å¦å­˜åœ¨ç©ºå€¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_rows_idx = housing.isnull().any(axis=1)\n",
    "print(null_rows_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> æŸ¥çœ‹è¨“ç·´é›†å«æœ‰ç©ºå€¼çš„è¡Œæ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_rows_idx.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.loc[null_rows_idx].head() # loc -> æ ¹æ“šè¡Œæˆ–åˆ—çš„æ¨™ç±¤é¸æ“‡æ•¸æ“š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ä¸Ÿæ‰ç›¸æ‡‰çš„å€åŸŸï¼šåˆªé™¤æœ‰ç¼ºå¤±å€¼çš„è¡Œï¼Œæ•´å€‹å€åŸŸçš„æ•¸æ“šè¢«åˆªé™¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_option1 = housing.copy()\n",
    "housing_option1.dropna(subset = [\"total_bedrooms\"], inplace=True) # Option 1\n",
    "housing_option1.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ä¸Ÿæ‰æ•´å€‹å±¬æ€§ï¼Œæœƒå¤±å» \"total_bedrooms\" é€™å€‹ç‰¹å¾µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_option2 = housing.copy()\n",
    "housing_option2.drop(\"total_bedrooms\", axis = 1, inplace=True) # Option 2\n",
    "housing_option2.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> åˆ©ç”¨ç‰¹å®šçš„å€¼ï¼ˆä¸­ä½æ•¸ï¼‰å¡«å……å’Œæ›¿æ›ç¼ºå¤±å€¼ï¼ŒåŒæ™‚ä¿ç•™æœ‰ç¼ºå¤±å€¼çš„è¡Œä¸¦å¡«ä¸Šæ•¸å€¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_option3 = housing.copy()\n",
    "\n",
    "# è¨ˆç®—è¨“ç·´é›†ä¸Šçš„ä¸­ä½æ•¸\n",
    "median = housing[\"total_bedrooms\"].median()\n",
    "housing_option3[\"total_bedrooms\"].fillna(median, inplace = True) # Option 3\n",
    "\n",
    "housing_option3.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Imputer** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleImputer çš„ `strategy åƒæ•¸` å¯ä»¥è¨­ç½®ç‚ºä¸åŒçš„å€¼ï¼Œä»¥æŒ‡å®šå°ç¼ºå¤±å€¼çš„å¡«å……ç­–ç•¥ï¼š\n",
    "- **median**ï¼šä½¿ç”¨ç‰¹å¾µçš„å¹³å‡å€¼ä¾†å¡«å……ç¼ºå¤±å€¼\n",
    "\n",
    "- **mean**ï¼šä½¿ç”¨ç‰¹å¾µçš„å‡å€¼ä¾†å¡«å……ç¼ºå¤±å€¼\n",
    "\n",
    "- **constant**ï¼šä½¿ç”¨æŒ‡å®šçš„å¸¸æ•¸å€¼ä¾†å¡«å……ç¼ºå¤±å€¼\n",
    "\n",
    "    - æŒ‡å®š fill_value åƒæ•¸ï¼š SimpleImputer(strategy=\"constant\", fill_value=0)ï¼Œé€™æ¨£ç¼ºå¤±å€¼å°±æœƒè¢«å¡«å……ç‚º 0ã€‚\n",
    "\n",
    "- **most_frequent**ï¼šä½¿ç”¨æœ€å¸¸è¦‹çš„ç‰¹å¾µå€¼ä¾†å¡«å……ç¼ºå¤±å€¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš ï¸ fit() èˆ‡ transform() æ–¹æ³•çš„æ¯”è¼ƒï¼š\n",
    "\n",
    "- å‰è€…åªè¼¸å…¥æ•¸æ“šä¸é€²è¡Œè½‰æ›ï¼Œå¾Œè€…å°‡è¼¸å…¥çš„æ•¸æ“šé€²è¡Œè½‰æ›ã€‚\n",
    "\n",
    "- transform() é€²è¡Œåœ¨ fit() çš„åŸºç¤ä¸Š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš ï¸ åœ¨ SimpleImputer ä¸­ä½¿ç”¨ `fit_transform` æ–¹æ³•ï¼š\n",
    "- SimpleImputer æ˜¯ä¸€å€‹è½‰æ›å™¨\n",
    "\n",
    "- fit_transform æ–¹æ³•å¯ä»¥æ“¬åˆæ•¸æ“šå’Œé€²è¡Œè½‰æ›\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> å°æ•¸æ“šé€²è¡Œç¼ºå¤±å€¼çš„å¡«å……ï¼Œç¢ºä¿æ¨¡å‹ä¸æœƒå› ç‚ºç¼ºå¤±å€¼è€Œå‡ºç¾éŒ¯èª¤ï¼Œä¸¦å„²å­˜ `æ¯å€‹ç‰¹å¾µçš„ä¸­ä½æ•¸(Median)`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "# å°‡æ¯å€‹å±¬æ€§çš„ç¼ºå¤±å€¼æ›¿æ›ç‚ºè©²å±¬æ€§çš„ä¸­ä½æ•¸\n",
    "imputer = SimpleImputer(strategy= \"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆªé™¤ \"ocean_proximity\" åˆ—ï¼Œç¢ºä¿ housing_num åªåŒ…å«æ•¸å­—è³‡æ–™\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis = 1)\n",
    "# è¨“ç·´é›†ä¸­çš„æ•¸å€¼å»æ“¬å’Œæ¯å€‹ç‰¹å¾µçš„ä¸­ä½æ•¸\n",
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> æ¯å€‹ç‰¹å¾µçš„å¹³å‡å€¼ï¼ˆä¸åŒ…å«\"ocean_proximity\"ï¼‰æœƒå„²å­˜åœ¨ statistics_  array ä¸­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = imputer.transform(housing_num) # MatrixLike -> æ¶µè“‹è¨±å¤šæ•¸æ“šçµæ§‹çš„è®Šæ•¸ï¼ˆNumpy, DataFrame, spareMatrix...ï¼‰\n",
    "# å°‡ x çµæœæ”¾å› DataFrame ä¸­\n",
    "housing_tr = pd.DataFrame(x, index=housing_num.index, columns= housing_num.columns)\n",
    "housing_tr[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ **Handling Text and Categorical Attributes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> å…ˆå‰éƒ½åœ¨è™•ç†æ•¸å­—å±¬æ€§ï¼ˆfloat64ï¼‰ï¼Œä½†åœ¨ housing è³‡æ–™é›†ä¸­æœ‰ä¸€å€‹**æ–‡å­—å±¬æ€§ï¼ˆobjectï¼‰**-> \"ocean_proximity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "housing_cat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ’¡ **Numpy array**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"ocean_proximity\" åº•ä¸‹çš„å€¼éä»»æ„æ–‡å­—ï¼Œå…¶å¯èƒ½çš„å€¼æœ‰é™ï¼Œæ¯å€‹å€¼ä»£è¡¨ä¸€å€‹é¡åˆ¥(object)ã€‚\n",
    "\n",
    "> ä½¿ç”¨Scikit-Learnçš„OrdinalEncoderé¡(Numpy array)ï¼Œå¯ä»¥å°‡æ­¤é¡åˆ¥çš„ã€Œ**æ–‡å­—è½‰æ›ç‚ºæ•¸å­—**ã€ï¼Œè®“æ©Ÿå™¨å¯ä»¥æ›´å¥½è®€å–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get category list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ’¡ **Sparse matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> å¯ä»¥è§€å¯Ÿåˆ° \"housing_cat_1hot\" ç‚ºä¸€ç¨€ç–çŸ©é™£ï¼Œè€Œä¸æ˜¯ Numpy é™£åˆ—ï¼š\n",
    "\n",
    "1. ç¨€ç–çŸ©é™£ï¼š ç¨€ç–çŸ©é™£ä½¿ç”¨ä¸€ç¨®å£“ç¸®çš„å½¢å¼ï¼Œåƒ…å„²å­˜éé›¶å…ƒç´ çš„å€¼æˆ–ä½ç½®ï¼ŒåŒ…æ‹¬ COOï¼ˆåæ¨™ï¼‰ã€CSRï¼ˆå£“ç¸®è¡Œï¼‰ã€CSCï¼ˆå£“ç¸®åˆ—ï¼‰ã€‚\n",
    "\n",
    "2. Numpy é™£åˆ—ï¼š Numpy é™£åˆ—ä»¥å¯†é›†ï¼ˆdenseï¼‰çš„å½¢å¼å­˜å„²ï¼Œå³æ‰€æœ‰å…ƒç´ éƒ½æœ‰å›ºå®šçš„ä½ç½®ã€‚\n",
    "\n",
    "â ç¸½çµä¾†èªªï¼Œç¨€ç–çŸ©é™£ä¸»è¦ç”¨æ–¼è™•ç†ã€Œæ•¸æ“šä¸­åŒ…å«å¤§é‡é›¶å€¼ã€çš„æƒ…æ³ï¼Œä»¥æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨ï¼›Numpy é™£åˆ—å‰‡æ˜¯ä¸€ç¨®é€šç”¨çš„å¤šç¶­æ•¸çµ„çµæ§‹ï¼Œå°æ–¼ã€Œå¯†é›†çš„æ•¸æ“šã€è¡¨ç¤ºéå¸¸æœ‰æ•ˆã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### â­ï¸ **OneHotEncoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One-hot encoding æ˜¯ä¸€ç¨®å°‡é¡åˆ¥å‹è®Šæ•¸è½‰æ›ç‚ºäºŒé€²åˆ¶è¡¨ç¤ºçš„æ–¹æ³•ï¼Œä¸¦ä¸”åªæœ‰å…¶ä¸­ä¸€å€‹ç‰¹å¾µçš„å€¼ç‚º 1ï¼Œå…¶é¤˜ç‰¹å¾µçš„å€¼å‡ç‚º 0ã€‚\n",
    "1. æ¯å€‹é¡åˆ¥è®Šæ•¸çš„æ¯å€‹å”¯ä¸€å€¼éƒ½å°‡è¢«æ˜ å°„åˆ°ä¸€å€‹æ–°çš„äºŒé€²åˆ¶ç‰¹å¾µã€‚\n",
    "\n",
    "2. å°æ–¼æ¯å€‹æ¨£æœ¬ï¼Œåƒ…æœ‰å±¬æ–¼å…¶åŸå§‹é¡åˆ¥çš„äºŒé€²åˆ¶ç‰¹å¾µçš„å€¼ç‚º 1ï¼Œå…¶é¤˜ç‰¹å¾µçš„å€¼å‡ç‚º 0ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> åœ¨ training set ä½¿ç”¨ \"fit_transform()\" æ–¹æ³•ï¼Œéœ€è¦å…ˆã€Œé©æ‡‰ã€OneHotEncoder å° training dataã€‚\n",
    "1. æª¢æ¸¬é¡åˆ¥è®Šæ•¸çš„å”¯ä¸€å€¼ï¼Œç‚ºæ¯å€‹å”¯ä¸€å€¼åˆ†é…ä¸€å€‹æ–°çš„äºŒé€²åˆ¶ç‰¹å¾µï¼ˆ0 or 1ï¼‰ã€‚\n",
    "\n",
    "2. å°‡training data è½‰æ›æˆæ–°çš„äºŒé€²åˆ¶ç‰¹å¾µ\n",
    "\n",
    "3. é€™æ¨£æ¨¡å‹å°±èƒ½å¤ ç†è§£é€™äº›ç‰¹å¾µï¼ŒOneHotEncoder ä¹Ÿå­¸åˆ°äº†è½‰æ›è¦å‰‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_1hot.toarray() # Sparse matrix -> Numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¨­å®š sparse = False -> \"fit_transform()\"æ–¹æ³•ç›´æ¥è¿”å›ä¸€å€‹å¯†é›† NumPy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder_1 = OneHotEncoder(sparse_output = False)\n",
    "housing_cat_1hot = cat_encoder_1.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use OneHotEncoder kit to list categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder_1.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Pandas has a function called \"get_dummies()\" \n",
    "1. æ•¸æ“šåŒ…å«é¡åˆ¥è®Šæ•¸ï¼ˆé¡è‰²ã€å¤©æ°£ã€åœ°å€ç­‰ï¼‰ä¸”è®Šæ•¸ç‚ºã€Œå­—ä¸²å½¢å¼ã€ï¼Œæ©Ÿå™¨å­¸ç¿’æ¨¡å‹å¯èƒ½éœ€è¦ã€Œæ•¸å€¼å½¢å¼ã€çš„ inputã€‚\n",
    "\n",
    "2. æ¯å€‹é¡åˆ¥å¯ä»¥æˆç‚ºæ–°çš„äºŒé€²åˆ¶ç‰¹å¾µåˆ—ï¼Œå…¶å€¼ç‚º 0 æˆ– 1ã€‚\n",
    "\n",
    "3. å°‡ \"ocean_proximity\" åˆ—ä¸­çš„é¡åˆ¥è®Šæ•¸è½‰æ›ä»¥äºŒé€²åˆ¶è¡¨ç¤ºï¼Œå°‡çµæœåˆä½µå›åŸå§‹ DataFrameã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame({\"ocean_proximity\": [\"INLAND\", \"NEAR BAY\", 'NEAR OCEAN']})\n",
    "result = pd.get_dummies(df_test) # ä½¿ç”¨ get_dummies() å°‡é¡åˆ¥è®Šæ•¸é€²è¡Œ one-hot ç·¨ç¢¼\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> åœ¨æ¸¬è©¦é›†æˆ–å…¶ä»–æ–°æ•¸æ“šä¸Šä½¿ç”¨ \"transform()\"ï¼Œè¡¨ç¤ºä½¿ç”¨ä¹‹å‰å·²ç¶“ã€Œé©æ‡‰ã€éçš„ OneHotEncoder çš„è½‰æ›è¦å‰‡ä¾†å°æ–°è³‡æ–™é€²è¡Œè½‰æ›ã€‚\n",
    "1. fit_transform() ç”¨æ–¼å­¸ç¿’å’Œè½‰æ›è¨“ç·´æ•¸æ“š\n",
    "2. transform() æ˜¯ä½¿ç”¨ä¹‹å‰å­¸åˆ°çš„è½‰æ›è¦å‰‡ä¾†è½‰æ›æ–°çš„æ•¸æ“šï¼ˆå¯ä»¥æ‡‰ç”¨åœ¨æœªçŸ¥æ•¸æ“šï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder_1.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed get_dummies() a DataFrame containing an unknown category (e.g., \"<2H OCEAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_unknown = pd.DataFrame({\"ocean_proximity\": [\"<2H OCEAN\", \"ISLAND\"]})\n",
    "pd.get_dummies(df_test_unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è™•ç†æœªçŸ¥é¡åˆ¥ \"<2H OCEAN\"ï¼ŒOneHotEncoder èƒ½å¤ æª¢æ¸¬æœªçŸ¥é¡åˆ¥ä¸¦æå‡ºç•°å¸¸ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder_1.handle_unknown = \"ignore\" \n",
    "cat_encoder_1.transform(df_test_unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder_1.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder_1.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = pd.DataFrame(cat_encoder_1.transform(df_test_unknown),\n",
    "                         columns = cat_encoder_1.get_feature_names_out(),\n",
    "                         index = df_test_unknown.index)\n",
    "print(df_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ **Feature Scaling and Transformation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- åœ¨è™•ç†æ•¸æ“šæ™‚ï¼Œæœ‰çš„æ™‚å€™æ•¸å€¼ç‰¹å¾µçš„å¤§å°å·®ç•°å¾ˆå¤§ï¼Œé€™å¯èƒ½æœƒå½±éŸ¿åˆ°æ¨¡å‹çš„è¡¨ç¾ï¼Œå› ç‚ºæ¨¡å‹å¯èƒ½æœƒã€Œ**åå‘**ã€é—œæ³¨å€¼è¼ƒå¤§çš„ç‰¹å¾µã€‚\n",
    "- èˆ‡æ‰€æœ‰è½‰æ›ä¸€æ¨£ï¼Œé‡è¦çš„æ˜¯å°‡ç¸®æ”¾å™¨åªé©åˆè¨“ç·´è³‡æ–™ï¼Œè€Œä¸æ˜¯æ•´å€‹è³‡æ–™é›†ï¼ˆåŒ…æ‹¬æ¸¬è©¦é›†ï¼‰ã€‚\n",
    "\n",
    "- ä½¿ç”¨ `ç‰¹å¾µç¸®æ”¾`ï¼Œç›®çš„æ˜¯å°‡æ‰€æœ‰ç‰¹å¾µçš„æ•¸å€¼ `èª¿æ•´åˆ°ç›¸åŒçš„å°ºåº¦`ï¼š\n",
    "    1. **Min-max scaling**ï¼šå°‡æ‰€æœ‰ç‰¹å¾µçš„å€¼ç¸®æ”¾åˆ°ä¸€å€‹æŒ‡å®šçš„ç¯„åœå…§ï¼Œé€šå¸¸åœ¨ 0 åˆ° 1 ä¹‹é–“ã€‚\n",
    "    2. **Standardization**ï¼šå°‡ç‰¹å¾µçš„å€¼è½‰æ›ç‚ºæ¨™æº–å¸¸æ…‹åˆ†ä½ˆ(normal distribution)ï¼Œä½¿å®ƒå€‘çš„å¹³å‡å€¼ç‚º 0ï¼Œæ¨™æº–å·®ç‚º 1ã€‚ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> fit() or fit_transform() only use for the `training set`.\n",
    "\n",
    "> If you have a **trained scaler**, you can use **it** to transform() any other set, including testing set, validation set and new data. \n",
    "\n",
    "â Trained scaler æŒ‡çš„æ˜¯ã€Œå·²ç¶“ã€ä½¿ç”¨ç‰¹å®šè³‡æ–™é›†é€²è¡Œé©æ‡‰(fit)çš„ç‰¹å¾µç¸®å™¨ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Min-max scaling (Normalization)**\n",
    "\n",
    "1. å°æ¯å€‹å±¬æ€§ï¼Œå€¼æœƒè¢«ç§»å‹•ä¸¦é‡æ–°ç¸®æ”¾ï¼Œè®“å®ƒå€‘æœ€çµ‚ä»‹æ–¼ 0 åˆ° 1 ä¹‹é–“ã€‚\n",
    "\n",
    "2. **é€éæ¸›å»æœ€å°å€¼ä¸¦é™¤ä»¥æœ€å¤§å€¼æ¸›å»æœ€å°å€¼**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Scikit-Learn provides a transformer called \"MinMaxScaler\"\n",
    "\n",
    "`It has a \"feature_range\" hyperparameterï¼ˆè¶…åƒæ•¸ï¼‰`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scalar = MinMaxScaler(feature_range=(-1,1))\n",
    "housing_num_min_max_scaled = min_max_scalar.fit_transform(housing_num) # housing_num æ˜¯ç´”æ•¸å€¼è¨“ç·´é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Standardization**\n",
    "1. **æ¸›å»å¹³å‡å€¼**ï¼šå°‡æ¯å€‹æ•¸å€¼æ¸›å»è¨“ç·´é›†çš„å¹³å‡å€¼ï¼Œé€™æ¨£æ¨™æº–åŒ–å¾Œçš„å¹³å‡å€¼å°±æœƒè®Šæˆé›¶ã€‚\n",
    "\n",
    "2. **é™¤ä»¥æ¨™æº–å·®**ï¼šå°‡çµæœé™¤ä»¥è¨“ç·´é›†çš„æ¨™æº–å·®ï¼Œé€™æ¨£æ¨™æº–åŒ–å¾Œçš„æ¨™æº–å·®å°±æœƒè®Šæˆ 1ã€‚\n",
    "\n",
    "3. æ¨™æº–åŒ–ä¸æœƒå°‡å€¼è¨­ç«‹åˆ°ç‰¹å®šç¯„åœï¼Œé€™å°ä¸€äº› ML æ¼”ç®—æ³•ä¾†èªªå¯èƒ½æ˜¯ä¸€å€‹å•é¡Œï¼ˆä¾‹å¦‚ï¼Œç¥ç¶“ç¶²è·¯é€šå¸¸æœŸæœ›è¼¸å…¥å€¼å¾0åˆ°1ä¸ç­‰ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ç¨€ç–çŸ©é™£ï¼ˆSparse Matrixï¼‰çš„è™•ç†**\n",
    "\n",
    "1. ç¨€ç–çŸ©é™£ä¸­æœ‰å¾ˆå¤šé›¶å€¼ï¼Œå¦‚æœç›´æ¥ä½¿ç”¨æ¨™æº–çš„ç‰¹å¾µç¸®æ”¾æ–¹å¼å¯èƒ½æœƒç ´å£çŸ©é™£çš„ç¨€ç–çµæ§‹ã€‚\n",
    "\n",
    "2. `StandardScaler(with_mean = Fasle)`ä½¿ç”¨ StandardScalar ç”¨æ–¼æ¨™æº–åŒ–ï¼Œä¸¦å°‡å…¶è¶…åƒæ•¸è¨­ç‚ºFalseã€‚\n",
    "\n",
    "3. é€™æ¨£ä»£è¡¨è³‡æ–™åœ¨é€²è¡Œé™¤æ³•æ™‚åªä½¿ç”¨æ¨™æº–å·®ï¼Œè€Œä¸æœƒæ¸›æ‰å¹³å‡å€¼ï¼Œå¯ä»¥ä¿ç•™ç¨€ç–çŸ©é™£ä¸­çš„é›¶å€¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "housing_num_std_scaled = std_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Input Featureâ€™s distributionï¼ˆè¼¸å…¥ç‰¹å¾µï¼‰**\n",
    "è¼¸å…¥ç‰¹å¾µæ˜¯ç”¨ä¾†è¨“ç·´æ©Ÿå™¨å­¸ç¿’æ¨¡å‹çš„æ•¸æ“šçš„å±¬æ€§\n",
    "\n",
    "ç•¶ input feature æ•¸æ“šåˆ†ä½ˆæ˜¯ heavy tail or deviation from normal distribution æ™‚ï¼š\n",
    "\n",
    "1.  Heavy tail è™•ç†é‡å°¾åˆ†å¸ƒ\n",
    "\n",
    "2. Multimodal distribution å¤šå³°åˆ†ä½ˆè™•ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1. Heavy tail è™•ç†é‡å°¾åˆ†å¸ƒ**\n",
    "\n",
    "å¤§å¤šæ•¸å€¼éƒ½é›†ä¸­åœ¨ä¸€å€‹å°ç¯„åœå…§ï¼Œå¯ä»¥å…ˆå°ç‰¹å¾µé€²è¡Œè½‰æ›ï¼Œç¸®å°é‡å°¾çš„å½±éŸ¿ã€‚\n",
    "\n",
    "ç”¨å¹³æ–¹æ ¹æˆ–å°‡ç‰¹å¾µæå‡åˆ°0åˆ°1ä¹‹é–“çš„æŸå€‹å†ªæ¬¡æ–¹ï¼Œæˆ–è€…å°æ•¸(log)è®Šæ›ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. å°‡è³‡æ–™é›†ç‰¹å¾µæ›ç®—ç‚ºå¹³æ–¹æ ¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"population\"].apply(np.sqrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. å¹‚å¾‹åˆ†ä½ˆæ›ç®—ç‚º log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"population\"].apply(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **åˆ†æ¡¶ï¼ˆBucketizingï¼‰**\n",
    "\n",
    "å°‡å…¶åˆ†ç‚ºå¤§è‡´ç›¸ç­‰å¤§å°çš„æ¡¶ï¼ˆå€é–“ï¼‰ï¼Œç„¶å¾Œå°‡æ¯å€‹ç‰¹å¾µå€¼æ›¿æ›ç‚ºå…¶æ‰€å±¬`æ¡¶çš„index`ã€‚\n",
    "\n",
    "é€™å‰µå»ºäº†ä¸€å€‹`æ¥è¿‘å‡å‹»åˆ†ä½ˆ`çš„ç‰¹å¾µï¼Œå°±ä¸éœ€è¦æ›´é€²ä¸€æ­¥çš„ç¸®æ”¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(8, 3), sharey=True)\n",
    "housing[\"population\"].hist(ax=axs[0], bins=50)\n",
    "housing[\"population\"].apply(np.log).hist(ax=axs[1], bins=50)\n",
    "axs[0].set_xlabel(\"Population\")\n",
    "axs[1].set_xlabel(\"Log of population\")\n",
    "axs[0].set_ylabel(\"Number of districts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2. Multimodal distribution è™•ç†å¤šå³°åˆ†ä½ˆ**\n",
    "1. **åˆ†æ¡¶ï¼ˆBucketizingï¼‰**ï¼šå°‡å…¶åˆ†ç‚ºå¤§è‡´ç›¸ç­‰å¤§å°çš„æ¡¶ï¼Œç„¶å¾ŒæŠŠæ¯å€‹é¡åˆ¥æ›¿æ›ç‚ºå…¶æ‰€å±¬ `æ¡¶çš„index` è€Œä¸æ˜¯æ•¸å€¼ã€‚ä½¿ç”¨ OneHotencoder\n",
    "\n",
    "2. **ç›¸ä¼¼çš„é¡åˆ¥ç‰¹å¾µ**ï¼šå°æ–¼å¤šå³°åˆ†ä½ˆï¼Œé‚„å¯ä»¥æ·»åŠ è¡¨ç¤ºç‰¹å®šå³°å€¼ç›¸ä¼¼æ€§çš„ç‰¹å¾µã€‚ä½¿ç”¨ `å¾‘å‘åŸºå‡½æ•¸ï¼ˆRBFï¼‰` è¨ˆç®—ç›¸ä¼¼æ€§ã€‚\n",
    "\n",
    "    ( è¼¸å‡ºå€¼æœƒéš¨è‘—è¼¸å…¥å€¼é é›¢å›ºå®šé»è€Œå‘ˆæŒ‡æ•¸ç´šåˆ¥éæ¸› )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "# æ¸¬é‡å±‹é½¡ä¸­ä½æ•¸èˆ‡ 35 ä¹‹é–“çš„ç›¸ä¼¼æ€§\n",
    "age_simil_35 = rbf_kernel(housing[[\"housing_median_age\"]], [[35]], gamma = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **æ•æ‰æˆ¿å±‹å¹´é½¡ä¸­ä½æ•¸èˆ‡ç‰¹å®šå¹´é½¡å€¼ä¹‹é–“çš„ç›¸ä¼¼æ€§**\n",
    "\n",
    "ä½¿ç”¨ `RBF kernel` ä¾†è¨ˆç®—ç‰¹å¾µé–“çš„ç›¸ä¼¼æ€§ã€‚å­˜åœ¨å¤šå³°åˆ†ä½ˆä¸­çš„å³°å€¼ï¼Œæ–°çš„ç‰¹å¾µåœ¨æˆ¿å±‹å¹´é½¡ä¸­ä½æ•¸ç‚º 35 æ™‚é”åˆ°å³°å€¼ï¼Œæ­¤åœ–å¯ä»¥äº†è§£è©²ç‰¹å®šå¹´é½¡ç¯„åœæ˜¯å¦èˆ‡è¼ƒä½åƒ¹æ ¼ç›¸é—œè¯ã€‚\n",
    "\n",
    "1. è¼ƒå°çš„ gamma å€¼å°è‡´ç›¸ä¼¼æ€§çš„å½±éŸ¿ç¯„åœè®Šå¤§ã€‚é€™æ„å‘³è‘—æ›´é çš„æ•¸æ“šé»å°ç›¸ä¼¼æ€§çš„è²¢ç»ä¹Ÿæ¯”è¼ƒå¤§ï¼ŒKernel çš„å½¢ç‹€è¼ƒç‚ºå¹³ç·©ã€‚\n",
    "\n",
    "2. è¼ƒå¤§çš„ gamma å€¼å°è‡´ç›¸ä¼¼æ€§çš„å½±éŸ¿ç¯„åœè®Šå°ã€‚é€™æ„å‘³è‘—åªæœ‰è¼ƒè¿‘çš„æ•¸æ“šé»å°ç›¸ä¼¼æ€§çš„è²¢ç»è¼ƒå¤§ï¼ŒKernel çš„å½¢ç‹€è¼ƒç‚ºå°–å³­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = np.linspace(housing[\"housing_median_age\"].min(),\n",
    "                   housing[\"housing_median_age\"].max(),\n",
    "                   500).reshape(-1, 1)\n",
    "gamma1 = 0.1\n",
    "gamma2 = 0.03\n",
    "rbf1 = rbf_kernel(ages, [[35]], gamma = gamma1) # gamma æ§åˆ¶äº† RBF kernel çš„å½¢ç‹€\n",
    "rbf2 = rbf_kernel(ages, [[35]], gamma = gamma2)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel(\"Housing median age\")\n",
    "ax1.set_ylabel(\"Number of districts\")\n",
    "ax1.hist(housing[\"housing_median_age\"], bins = 50)\n",
    "\n",
    "ax2 = ax1.twinx()  # create a twin axis that shares the same x-axis\n",
    "color = \"blue\"\n",
    "ax2.plot(ages, rbf1, color=color, label=\"gamma = 0.10\")\n",
    "ax2.plot(ages, rbf2, color=color, label=\"gamma = 0.03\", linestyle=\"--\")\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_ylabel(\"Age similarity\", color=color)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Target Valuesï¼ˆç›®æ¨™è®Šæ•¸ï¼‰**\n",
    "\n",
    "ç›®æ¨™è®Šæ•¸æ˜¯åœ¨ç›£ç£å­¸ç¿’ä¸­ï¼Œæˆ‘å€‘å¸Œæœ›æ¨¡å‹é æ¸¬æˆ–åˆ†é¡çš„æ•¸å€¼æˆ–æ¨™ç±¤ã€‚\n",
    "\n",
    "ç•¶ç›®æ¨™è®Šæ•¸çš„åˆ†ä½ˆç‚º heavy tail or heavy tail or deviation from normal distribution æ™‚ï¼š\n",
    "\n",
    "1. ç›´æ¥è½‰æ›ç›®æ¨™è®Šæ•¸\n",
    "\n",
    "2. ä½¿ç”¨ TransformedTargetRegressor ç°¡åŒ–è½‰æ›"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. ç›´æ¥è½‰æ›ç›®æ¨™è®Šæ•¸**\n",
    "\n",
    "- å°ç›®æ¨™è®Šæ•¸çš„è½‰æ›å’Œé‚„åŸéç¨‹è¼ƒç‚ºæ˜ç¢ºï¼Œèƒ½å¤ æ›´éˆæ´»åœ°è™•ç†ã€‚\n",
    "\n",
    "- å¯ä»¥ `æ‰‹å‹•æ§åˆ¶` è½‰æ›çš„éç¨‹ï¼Œæ ¹æ“šéœ€æ±‚èª¿æ•´è½‰æ›å™¨çš„åƒæ•¸ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ç›´æ¥ä½¿ç”¨ StandardScaler é€²è¡Œç›®æ¨™è®Šæ•¸çš„ç¸®æ”¾\n",
    "target_scaler = StandardScaler()\n",
    "scaled_labels = target_scaler.fit_transform(housing_labels.to_frame())\n",
    "\n",
    "# ä½¿ç”¨ç·šæ€§å›æ­¸æ¨¡å‹æ“¬åˆ(fit)ç¸®æ”¾å¾Œçš„ç›®æ¨™è®Šæ•¸\n",
    "model = LinearRegression()\n",
    "model.fit(housing[[\"median_income\"]], scaled_labels)\n",
    "\n",
    "# ç•¶éœ€è¦é€²è¡Œé æ¸¬æ™‚ï¼Œæ¨¡å‹æœƒå‘¼å«å›æ­¸æ¨¡å‹çš„ predict() æ–¹æ³•\n",
    "some_new_data = housing[[\"median_income\"]].iloc[:5]  # pretend this is new data\n",
    "scaled_predictions = model.predict(some_new_data)\n",
    "\n",
    "# ä½¿ç”¨ transformer çš„ inverse_transform() æ–¹æ³•é‚„åŸé æ¸¬å€¼çš„ç¸®æ”¾\n",
    "predictions = target_scaler.inverse_transform(scaled_predictions)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. ä½¿ç”¨ TransformedTargetRegressor ç°¡åŒ–è½‰æ›**\n",
    "\n",
    "- åœ¨é æ¸¬éšæ®µå¯ä»¥æ–¹ä¾¿çš„è™•ç†è½‰æ›å‰å¾Œçš„æ•¸æ“š\n",
    "\n",
    "- åœ¨æ¨¡å‹æ“¬åˆå’Œé æ¸¬éç¨‹ä¸­ï¼Œè‡ªå‹•è™•ç†ç›®æ¨™è®Šæ•¸çš„è½‰æ›å’Œé‚„åŸ\n",
    "\n",
    "- TransformedTargetRegressor æœƒ **è‡ªå‹•ä½¿ç”¨ transformer** å°ç›®æ¨™è®Šæ•¸é€²è¡Œç¸®æ”¾ï¼Œç„¶å¾Œ `ä½¿ç”¨ç¸®æ”¾å¾Œçš„ç›®æ¨™è®Šæ•¸` è¨“ç·´å›æ­¸æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "# çµ¦å®šå…©å€‹åƒæ•¸ 'ç·šæ€§å›æ­¸æ¨¡å‹' å’Œ 'ç›®æ¨™è®Šæ•¸çš„ transformer' çµæ ¸ä¸¦é€²è¡Œç›®æ¨™è®Šæ•¸è½‰æ›\n",
    "model = TransformedTargetRegressor(LinearRegression(), transformer = StandardScaler())\n",
    "\n",
    "# ç”¨åŸå§‹ç›®æ¨™è®Šæ•¸é€²è¡Œæ¨¡å‹æ“¬åˆï¼Œå®ƒæœƒã€Œè‡ªå‹•è™•ç†ã€ç›®æ¨™è®Šæ•¸çš„ç¸®æ”¾ã€‚\n",
    "model.fit(housing[[\"median_income\"]], housing_labels)\n",
    "\n",
    "# ä½¿ç”¨æ¨¡å‹é æ¸¬æ–°æ•¸æ“š\n",
    "predictions = model.predict(some_new_data)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ **Customer Transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a customer transformer â¡ï¸ æ”¹å–„ç‰¹å¾µçš„åˆ†ä½ˆï¼Œä½¿å…¶æ›´è¶¨è¿‘æ–¼ `å¸¸æ…‹åˆ†ä½ˆ(normal distribution)`ï¼Œå› ç‚ºæ¨¡å‹è™•ç†å¸¸æ…‹åˆ†ä½ˆçš„è¡¨ç¾è¼ƒå¥½ã€‚**\n",
    "\n",
    "Scikit-Learn ä¸­æä¾›è¨±å¤šæœ‰ç”¨çš„è½‰æ›å™¨ï¼Œåœ¨ä¸€äº›æƒ…æ³ä¸‹ï¼Œéœ€è¦æ’°å¯«è‡ªå·±çš„ Transformer ä¾†åŸ·è¡Œè‡ªå®šç¾©çš„è½‰æ›ã€æ¸…ç†æ“ä½œæˆ–çµåˆç‰¹å®šå±¬æ€§ã€‚\n",
    "\n",
    "â­ï¸ Transform contains å°æ•¸è½‰æ›ã€æ–¹æ ¹è½‰æ› â¡ï¸ ä½¿ç‰¹å¾µæ›´æ¥è¿‘å¸¸æ…‹åˆ†ä½ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ä½¿ç”¨ FunctionTransformer å‰µå»ºä¸åŒçš„è½‰æ›å™¨**\n",
    "å¾å°æ•¸è½‰æ›åˆ°è¨ˆç®—é«˜æ–¯ RBF ç›¸ä¼¼åº¦ï¼Œä»¥åŠçµ„åˆç‰¹å¾µçš„ä¾‹å­ï¼Œå¯ä»¥åœ¨ Scikit-Learn Pipeline ä¸­ä½¿ç”¨ï¼Œä»¥æ‡‰ç”¨ç›¸åŒçš„è½‰æ›é‚è¼¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> å°æŸå€‹ç‰¹å¾µï¼ˆé€™è£¡æ˜¯äººå£æ•¸ï¼‰é€²è¡Œå°æ•¸è½‰æ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# ä½¿ç”¨ NumPy çš„å°æ•¸å‡½æ•¸å’ŒæŒ‡æ•¸å‡½æ•¸ create ä¸€å€‹å°æ•¸è½‰æ›å™¨\n",
    "log_transformer = FunctionTransformer(np.log, inverse_func = np.exp)\n",
    "log_pop = log_transformer.transform(housing[[\"population\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> å‰µå»ºé«˜æ–¯RBFç›¸ä¼¼åº¦è½‰æ›å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨ˆç®—èˆ‡ä¹‹å‰ç›¸åŒçš„ RBF -> å»ºç«‹æ¸¬é‡å±‹é½¡ä¸­ä½æ•¸èˆ‡ 35 ä¹‹é–“çš„ç›¸ä¼¼æ€§çš„ transformer\n",
    "rbf_transformer = FunctionTransformer(rbf_kernel, kw_args = dict(Y = [[35.]], gamma = 0.1))\n",
    "age_simil_35 = rbf_transformer.transform(housing[[\"housing_median_age\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> å‰µå»ºåœ°ç†ä½ç½®ç›¸ä¼¼åº¦è½‰æ›å™¨\n",
    "\n",
    "- RBF Kernel æ˜¯éç·šæ€§å‡½æ•¸ï¼Œæ˜¯è¨ˆç®—åœ¨æŸå€‹å›ºå®šé»åˆ°å…¶ä»–é»çš„è·é›¢ä¸Šçš„æœ‰å…©å€‹ç›¸ä¼¼åº¦å€¼ã€‚\n",
    "- åœ¨ä¸€å€‹è·é›¢ä¸Š **å­˜åœ¨å…©å€‹ç›¸æ‡‰çš„è¼¸å‡ºå€¼**ï¼Œå› æ­¤ `ä¸å­˜åœ¨å–®ä¸€çš„é€†å‡½æ•¸`ï¼Œç„¡æ³•å°‡è¼¸å‡ºå€¼æº–ç¢ºé‚„åŸç‚ºåŸå§‹è¼¸å…¥ã€‚\n",
    "\n",
    "- åœ¨è¨ˆç®—ç›¸ä¼¼åº¦æ™‚ï¼Œä¸å€åˆ†è¼¸å…¥ä¸­çš„ç‰¹å¾µã€‚å¦‚æœæœ‰ä¸€å€‹å¤šå€‹ç‰¹å¾µçš„é™£åˆ—ï¼Œå®ƒæœƒä½¿ç”¨ `æ­æ°è·é›¢ï¼ˆ2Dè·é›¢ï¼‰`ä¾†è¨ˆç®—å’Œè¡¡é‡ç›¸ä¼¼åº¦ï¼ˆç²¾æº–åº¦é™ä½ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_coords = 37.7749, -122.41 # sf_coords å­˜æ”¾èˆŠé‡‘å±±çš„ç¶“ç·¯åº¦\n",
    "# ä½¿ç”¨rbf_kernelå‡½æ•¸ä½œç‚ºè½‰æ›å‡½æ•¸, kw_argsåƒæ•¸ -> ç”¨ä¾†å‚³éé™„åŠ çš„åƒæ•¸çµ¦rbf_kernelå‡½æ•¸ï¼ŒåŒ…å«å›ºå®šé»çš„åæ¨™ 'Y'ï¼ˆå³èˆŠé‡‘å±±çš„åæ¨™ï¼‰ã€gammaå€¼ã€‚\n",
    "sf_transformer = FunctionTransformer(rbf_kernel, kw_args = dict(Y = [sf_coords], gamma = 0.1))\n",
    "sf_simil = sf_transformer.transform(housing[[\"latitude\", \"longitude\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> å‰µå»ºä¸€å€‹å¯ä»¥è¨ˆç®—è¼¸å…¥ç‰¹å¾µçš„æ¯”ä¾‹çš„è½‰æ›å™¨\n",
    "\n",
    "- ç›®çš„æ˜¯ä½¿ç”¨ Customer transformer ä¾†å‰µé€ æ–°çš„ç‰¹å¾µï¼Œæ­¤ç‰¹å¾µæ˜¯åŸå§‹ç‰¹å¾µä¹‹é–“çš„æ¯”ä¾‹ï¼Œå¯èƒ½å¯ä»¥è®“æ¨¡å‹æ›´å¥½åœ°æ•æ‰ç‰¹å¾µä¹‹é–“çš„é—œä¿‚ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ lambda function è¨ˆç®—äº†è¼¸å…¥é™£åˆ—ä¸­ç¬¬ä¸€åˆ—å’Œç¬¬äºŒåˆ—ä¹‹é–“çš„æ¯”ä¾‹\n",
    "ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])\n",
    "# å°‡åŒ…å«å…©è¡Œå…©åˆ—çš„é™£åˆ—ï¼Œæ‡‰ç”¨åœ¨ ratio_transformer é€™å€‹è½‰æ›å™¨ä¸Š\n",
    "result = ratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Framework**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass  # no hyperparameters to tune\n",
    "    \n",
    "    def fit(self, X, y = None): # fit() é€šå¸¸ç”¨æ–¼åŸ·è¡Œå¿…è¦çš„åˆå§‹åŒ–\n",
    "        return self  # always return self\n",
    "    \n",
    "    def transform(self, X): # transform() åŸ·è¡Œå°æ•¸è½‰æ›\n",
    "        return np.log1p(X)  # log-transform the input array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sklearn.utils.validation è»Ÿé«”åŒ…åŒ…å«å¹¾å€‹å¯ç”¨æ–¼é©—è­‰è¼¸å…¥çš„å‡½å¼ã€‚\n",
    "\n",
    "- åœ¨ fit() æ–¹æ³•ä¸­è¨­å®š n_features_in_ï¼Œç¢ºä¿å‚³éçµ¦ transform() æˆ– predict() çš„è³‡æ–™å…·æœ‰é€™å€‹æ•¸é‡çš„ç‰¹å¾µã€‚\n",
    "\n",
    "- æ‰€æœ‰ estimators åœ¨**å‚³é** DataFrame æ™‚ï¼Œéƒ½æ‡‰åœ¨ fit() æ–¹æ³•ä¸­è¨­å®š `'feature_names_in_'`ã€‚\n",
    "\n",
    "- ç•¶ **transformer å¯ä»¥é€†è½‰**æ™‚ï¼Œéƒ½æ‡‰è©²æä¾› aget `'get_feature_names_out'` æ–¹æ³•ï¼Œä»¥åŠ `'aninverseinverse_transform'` æ–¹æ³•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "\n",
    "class StandardScalerClone(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, with_mean=True):\n",
    "        self.with_mean = with_mean\n",
    "\n",
    "    def fit(self, X, y = None): # y is required even through we don't use it !!\n",
    "        X = check_array(X) # check x  is whether not infinte float values.\n",
    "        self.mean_ = X.mean(axis = 0)\n",
    "        self.scale_ = X.std(axis = 0)\n",
    "        self.n_features_in_ = X.shape[1] # every estimator stores this in fit()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        assert self.n_features_in_ == X.shape[1] # promise self.n_features_in_ and X equal to number of feature.\n",
    "        if self.with_mean:\n",
    "            X = X - self.mean_\n",
    "        return X / self.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Customer transformer can use other estimators**\n",
    "\n",
    "- å¯ä»¥å¾ sklearn.utils.estimator_checks ä¸­å°‡ instance å‚³çµ¦ check_estimator() ä¾†æª¢æŸ¥è‡ªå®šç¾© estimators æ˜¯å¦å°Šé‡Scikit-Learnçš„APIã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        self.kmeans_ = KMeans(self.n_clusters, n_init=10, random_state=self.random_state)\n",
    "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
    "        return self  # always return self\n",
    "    \n",
    "    def  transform(self, X):\n",
    "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma= self.gamma) # ä½¿ç”¨ RBF è¨ˆç®—æ¯å€‹æ¨£æœ¬èˆ‡ clusters çš„ç›¸ä¼¼ç¨‹åº¦\n",
    "    \n",
    "    def get_feature_names_out(self, names = None):\n",
    "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)] # å›å‚³æ¯å€‹ç‰¹å¾µåç¨±ï¼Œæ¯å€‹åç¨±éƒ½ä»£è¡¨ä¸€å€‹ç›¸ä¼¼åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_simil = ClusterSimilarity(n_clusters= 10, gamma= 1., random_state= 42)\n",
    "similarities = cluster_simil.fit_transform(housing[[\"latitude\", \"longitude\"]], sample_weight = housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[:3].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_renamed = housing.rename(columns={\n",
    "    \"latitude\": \"Latitude\", \"longitude\": \"Longitude\",\n",
    "    \"population\": \"Population\",\n",
    "    \"median_house_value\": \"Median house value (á´œsá´…)\"})\n",
    "housing_renamed[\"Max cluster similarity\"] = similarities.max(axis = 1)\n",
    "\n",
    "housing_renamed.plot(kind = \"scatter\", x = \"Longitude\", y = \"Latitude\", grid = True,\n",
    "                     s = housing_renamed[\"Population\"] / 100, label = \"Population\",\n",
    "                     c=\"Max cluster similarity\",\n",
    "                     cmap=\"jet\", colorbar=True,\n",
    "                     legend=True, sharex=False, figsize=(8, 6))\n",
    "\n",
    "plt.plot(cluster_simil.kmeans_.cluster_centers_[:, 1],\n",
    "         cluster_simil.kmeans_.cluster_centers_[:, 0],\n",
    "         linestyle = \"\", color = \"black\", marker = \"X\", markersize = 15, label = \"Cluster centers\")\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ **Transformation Pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **A machine learning pipeline is used to help automate machine learning workflow.**\n",
    "\n",
    "- **ML pipelines are pipelines are iterative as every step is repeated to `continuously improve the accuracy` of the model and achieve a successful algorithm.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Simple operation of pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pipeline æ¯å€‹æ­¥é©Ÿçš„ **è½‰æ›å™¨**ï¼ˆtransformerï¼‰éƒ½æ‡‰å…·å‚™ `fit_transform() æ–¹æ³•` \n",
    "    - å› ç‚ºåœ¨ Pipeline ä¸­çš„æ¯å€‹æ­¥é©Ÿï¼Œéƒ½éœ€è¦é€²è¡Œä¸€äº›è³‡æ–™è½‰æ›ã€‚\n",
    "\n",
    "- Pipeline æœ€å¾Œä¸€å€‹æ­¥é©Ÿå¯ä»¥æ˜¯ `ä»»ä½•é¡å‹` çš„ä¼°è¨ˆå™¨ï¼ˆä¹Ÿå¯æ˜¯è½‰æ›å™¨ï¼‰\n",
    "    - å› ç‚ºåœ¨ä¸€å€‹Pipelineä¸­ï¼Œæœ€å¾Œä¸€å€‹æ­¥é©Ÿå¯èƒ½æ˜¯ä¸€å€‹é æ¸¬æ¨¡å‹ï¼Œæˆ–è€…å…¶ä»–ä¸éœ€è¦é€²è¡Œè½‰æ›çš„ä¼°è¨ˆå™¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. åˆ©ç”¨ scikit-learn çš„ Pipeline å»ºç«‹ Pipeline()\n",
    "\n",
    "2. å»ºç«‹åç‚º \"num_pipleline\" `è™•ç†æ•¸å€¼å‹ç‰¹å¾µ`ï¼ˆå¡«è£œç¼ºå¤±å€¼ï¼Œç„¶å¾Œå°é€™äº›ç‰¹å¾µé€²è¡Œæ¨™æº–åŒ–ï¼‰ç‚ºåŒ…å«å…©å€‹æ­¥é©Ÿçš„ Pipelineï¼š\n",
    "\n",
    "    - **impute** æ­¥é©Ÿï¼šä½¿ç”¨ SimpleImputer é€™å€‹è½‰æ›å™¨ï¼ˆtransformerï¼‰ï¼Œå°‡ç¼ºå¤±å€¼ä½¿ç”¨ä¸­ä½æ•¸é€²è¡Œå¡«å……ã€‚\n",
    "    \n",
    "    - **standardize** æ­¥é©Ÿï¼šä½¿ç”¨ StandardScaler é€™å€‹è½‰æ›å™¨ï¼Œå°ç‰¹å¾µé€²è¡Œæ¨™æº–åŒ–ï¼ˆå‡å€¼ç‚º0ã€å¹³æ–¹å·®ç‚º1çš„æ¨™æº–å¸¸æ…‹åˆ†ä½ˆï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"standardize\", StandardScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **å¦‚æœä¸æƒ³ç‚ºæ¯å€‹ step & transformers å‘½åï¼Œå‰‡ä½¿ç”¨ `make_pipeline()` æ–¹æ³•ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Scikit-Learn estimater æœƒå‘ˆç¾ä¸€äº’å‹•å¼åœ–è¡¨ï¼Œè¦–è¦ºåŒ– Pipeline æµç¨‹ã€‚** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "num_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- èª¿ç”¨ Pipeline çš„ fit() æ–¹æ³•æ™‚ï¼ŒæœƒæŒ‰é †åºåœ¨æ‰€æœ‰è½‰æ›å™¨ä¸Šä¾æ¬¡èª¿ç”¨ fit_transform()ã€‚\n",
    "\n",
    "- å°‡æ¯å€‹èª¿ç”¨çš„è¼¸å‡ºä½œç‚ºåƒæ•¸ï¼Œå‚³éçµ¦ä¸‹ä¸€å€‹èª¿ç”¨ï¼Œç›´åˆ°å‚³åˆ°æœ€çµ‚çš„ä¼°è¨ˆå™¨ï¼ˆestimatorï¼‰ã€‚\n",
    "\n",
    "- å°æ–¼**æœ€çµ‚çš„ä¼°è¨ˆå™¨**ï¼Œå®ƒ `åƒ…èª¿ç”¨ fit() æ–¹æ³•`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num_prepared = num_pipeline.fit_transform(housing_num) # housing_num æ˜¯ç´”æ•¸å€¼è¨“ç·´é›†\n",
    "housing_num_prepared[:2].round(2) #è¼¸å‡ºå‰å…©è¡Œï¼Œä¸¦å››æ¨äº”å…¥å–åˆ°å°æ•¸é»å¾Œå…©ä½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **å°‡ Pipeline è™•ç†æ•¸å€¼é¡å‹ç‰¹å¾µæ•¸æ“šè½‰æ›ç‚º DataFrame**\n",
    "- **å¯è¦–åŒ–åˆ†æ**ï¼šDataFrame å¯ä»¥æ›´å®¹æ˜“ä½¿ç”¨ matplotlib æˆ– seaborn ç­‰å‡½å¼åº«ç¹ªè£½ç›´æ–¹åœ–ã€æ•£ä½ˆåœ–ç­‰åœ–è¡¨ï¼Œè§€å¯Ÿæ•¸æ“šçš„åˆ†ä½ˆèˆ‡é—œè¯æ€§ã€‚\n",
    "\n",
    "- **å°æ¯”åŸå§‹æ•¸æ“š**ï¼šå°‡è™•ç†å¾Œçš„æ•¸æ“šè½‰æ›ç‚º DataFrame å¯ä»¥æ›´å®¹æ˜“å¾—æ¯”å°èˆ‡åŸå§‹æ•¸æ“šçš„å·®ç•°\n",
    "\n",
    "- **æ–¹ä¾¿å¾ŒçºŒæ“ä½œ**ï¼šå¾Œé¢çš„æ¨¡å‹è¨“ç·´ã€ç‰¹å¾µé¸æ“‡ç­‰æ“ä½œï¼Œé€šå¸¸éœ€è¦ä½¿ç”¨ DataFrame æˆ–é¡ä¼¼çš„æ•¸æ“šçµæ§‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing_num_prepared = pd.DataFrame(housing_num_prepared, columns= num_pipeline.get_feature_names_out(), index= housing_num.index)\n",
    "\n",
    "df_housing_num_prepared.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **æª¢æŸ¥ã€ç²å–å’Œä¿®æ”¹ Pipeline çš„çµæ§‹å’Œåƒæ•¸**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **.stepsï¼šåˆ—å‡º Pipeline ä¸­çš„æ­¥é©Ÿ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **å›å‚³ Pipeline ä¸­ç´¢å¼•ç‚º 1 çš„æ­¥é©Ÿ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **å›å‚³ Pipeline ä¸­æœ€å¾Œä¸€å€‹æ­¥é©Ÿã€Œä»¥å‰ã€çš„ SubPipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **.named_steps[\"\"]ï¼šæ ¹æ“šåç¨±æŸ¥è©¢ Pipeline ä¸­çš„ã€ŒæŒ‡å®šæ­¥é©Ÿã€ä¸¦å›å‚³**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline.named_steps[\"standardscaler\"] #å› ç‚ºæ˜¯ä½¿ç”¨ 'make_pipeline' å¹«æˆ‘å€‘ç”Ÿæˆæ¯å€‹æ­¥é©Ÿçš„åç¨±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **.set_params()ï¼šå‹•æ…‹èª¿æ•´ Pipeline æ­¥é©Ÿä¸­åƒæ•¸çš„æ–¹æ³•(ä¸éœ€é‡æ–°å»ºç«‹æ•´å€‹ Pipeline)**\n",
    "1. è¨­ç½®æ­¥é©Ÿçš„è¶…åƒæ•¸\n",
    "\n",
    "2. ä½¿ç”¨é›™åº•ç·š __ ä¾†æŒ‡å®šæ­¥é©Ÿçš„åç¨±å’Œè¶…åƒæ•¸çš„åç¨±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline.set_params(simpleimputer__strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **å»ºç«‹å°ˆæ¡ˆçš„ Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ä½¿ç”¨ \"ColumnTransformer\" è½‰æ›å™¨**\n",
    "- ä¸»è¦æ‡‰å°å…·æœ‰å¤šç¨®æ•¸å€¼å’Œé¡åˆ¥ç‰¹å¾µçš„æ•¸æ“šé›†\n",
    "\n",
    "- å¯ä»¥ç”¨æ–¼å°‡ä¸åŒçš„è½‰æ›ï¼Œæ‡‰ç”¨æ–¼æ•¸æ“šçš„ä¸åŒéƒ¨åˆ†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **æ ¹æ“šæŒ‡å®šçš„åç¨±åˆ†åˆ¥æŠŠå°æ‡‰çš„è½‰æ›å™¨æ‡‰ç”¨åˆ°ä¸åŒçš„ç‰¹å¾µé›†ä¸Š**\n",
    "1. å»ºç«‹å…©å€‹é™£åˆ—åˆ†åˆ¥æ”¾å…¥æ•¸å€¼å‹å’Œæ–‡å­—å‹ç‰¹å¾µ\n",
    "\n",
    "2. åˆ©ç”¨å·²å»ºç«‹çš„è™•ç†æ•¸å€¼å‹ç‰¹å¾µä¹‹ Pipeline\n",
    "\n",
    "3. å»ºç«‹ä¸€è™•ç†æ–‡å­—è½‰æ•¸å€¼å‹ç‰¹å¾µä¹‹ Pipeline\n",
    "\n",
    "4. å»ºç«‹ä¸€è‡ªå®šç¾©çš„è½‰æ›å™¨ preprocessing çš„ ColumnTransformer ç‰©ä»¶ï¼Œåˆ†åˆ¥å°‡æ•¸å€¼ç‰¹å¾µé›†æ‡‰ç”¨æ–¼ num_pipelineï¼Œå°‡åˆ†é¡ç‰¹å¾µé›†æ‡‰ç”¨æ–¼ cat_pipelineã€‚ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribute = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n",
    "cat_attribute = [\"ocean_proximity\"]\n",
    "\n",
    "cat_pipeline = make_pipeline(SimpleImputer(strategy=\"most_frequent\"), \n",
    "                             OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "      (\"num\", num_pipeline, num_attribute),\n",
    "    (\"cat\", cat_pipeline, cat_attribute),])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ä½¿ç”¨ \"make_column_transformer\" å’Œ \"make_column_selector\"**\n",
    "- \"make_column_transformer\" å¯ä»¥æ ¹æ“šç‰¹å¾µçš„è³‡æ–™é¡å‹ï¼ˆæ•¸å€¼å‹æˆ–é¡åˆ¥å‹ï¼‰ï¼ŒæŒ‡å®šä¸åŒçš„è½‰æ›æ­¥é©Ÿã€‚\n",
    "\n",
    "- \"make_column_selector\" æ˜¯è®“ä½ èƒ½å¤ å¾è³‡æ–™é¡å‹ä¸­ï¼Œè¼•é¬†åœ°å¾ DataFrame ä¸­é¸æ“‡æƒ³è¦çš„åˆ—ã€‚\n",
    "\n",
    "- å…¶ä¸­ \"dtype_include\" å’Œ \"dtype_exclude\" å…è¨±æ ¹æ“šæ•¸æ“šé¡å‹ä¾†é¸æ“‡æˆ–æ’é™¤ç‰¹å¾µåˆ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "\n",
    "preprocessing = make_column_transformer(\n",
    "    (num_pipeline, make_column_selector(dtype_include=np.number)), # ç•¶ DataFrame ä¸­æŸä¸€åˆ—çš„æ•¸æ“šé¡å‹æ˜¯æ•¸å­—ï¼ˆä¾‹å¦‚æ•´æ•¸æˆ–æµ®é»æ•¸ï¼‰ï¼Œé€™åˆ—å°±æœƒè¢«é¸ä¸­ï¼\n",
    "    (cat_pipeline, make_column_selector(dtype_include=object)), # ç•¶ DataFrame ä¸­æŸä¸€åˆ—çš„æ•¸æ“šé¡å‹æ˜¯æ–‡å­—ï¼ˆä¾‹å¦‚å­—ç¬¦ä¸²ï¼‰æˆ–å…¶ä»–å°è±¡ï¼Œé€™åˆ—å°±æœƒè¢«é¸ä¸­ï¼\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **å°‡åŸæœ‰ housing è¨“ç·´é›†é€²è¡Œé è™•ç† Pipeline**ï¼š\n",
    "1. **æ•¸å€¼å‹ç‰¹å¾µ (num_attribute) çš„è™•ç†**ï¼šä½¿ç”¨ num_pipeline é€²è¡Œæ•¸å€¼å‹ç‰¹å¾µçš„å¡«å……ç¼ºå¤±å€¼ï¼ˆä½¿ç”¨ä¸­ä½æ•¸ï¼‰ï¼Œä¸¦é€²è¡Œæ¨™æº–åŒ–ï¼ˆä½¿ç”¨æ¨™æº–å·®æ¨™æº–åŒ–ï¼‰ã€‚\n",
    "\n",
    "2. **é¡åˆ¥å‹ç‰¹å¾µ (cat_attribute) çš„è™•ç†**ï¼šä½¿ç”¨ cat_pipeline é€²è¡Œé¡åˆ¥å‹ç‰¹å¾µçš„å¡«å……ç¼ºå¤±å€¼ï¼ˆä½¿ç”¨çœ¾æ•¸ï¼‰å’Œ One-Hot Encodingã€‚\n",
    "\n",
    "3. ColumnTransformer å°‡é€™å…©å€‹ transformer å°æ‡‰åˆ°ç¬¦åˆçš„ç‰¹å¾µåˆ—ï¼Œä¸¦å°‡æ•´å€‹è³‡æ–™é›†è½‰æ›æˆé è™•ç†å¾Œçš„å½¢å¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared = preprocessing.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> å°‡ç¶“éé è™•ç†çš„è¨“ç·´é›† housing_prepared è½‰æ›ç‚º DataFrame æ ¼å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared_fr = pd.DataFrame(housing_prepared, columns=preprocessing.get_feature_names_out(), index=housing.index)\n",
    "housing_prepared_fr.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared_fr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Review to build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_ratio(X):\n",
    "    return X[:, [0]] / X[:, [1]]\n",
    "\n",
    "def ratio_name(function_transformer, feature_names_in):\n",
    "    return [\"ratio\"]  # feature names out\n",
    "\n",
    "def ratio_pipeline():\n",
    "    return make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
    "        StandardScaler())\n",
    "\n",
    "log_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n",
    "    StandardScaler())\n",
    "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
    "default_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
    "                                     StandardScaler())\n",
    "preprocessing = ColumnTransformer([\n",
    "        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n",
    "        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n",
    "        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n",
    "        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n",
    "                               \"households\", \"median_income\"]),\n",
    "        (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n",
    "        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n",
    "    ],\n",
    "    remainder=default_num_pipeline)  # one column remaining: housing_median_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared = preprocessing.fit_transform(housing)\n",
    "housing_prepared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– **Select and Train a Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ğŸš€ Training and Evaluating on the Training Set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Linear Regression**\n",
    "\n",
    "åœ¨ä¸€å †æ•¸æ“šä¸­ï¼Œå¸Œæœ› `æ‰¾åˆ°ä¸€æ¢ç›´ç·š`ï¼Œé€™æ¢ç›´ç·šèƒ½å¤ æœ€å¥½åœ°å»æ“¬åˆé€™äº›æ•¸æ“š â¡ é€™æ¢ç›´ç·šçš„æ–¹ç¨‹å¼å°±æ˜¯ç·šæ€§å›æ­¸æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> å»ºç«‹ç·šæ€§å›æ­¸æ¨¡å‹ï¼ˆä¹‹å¾Œå¯ä»¥åˆ©ç”¨æ­¤æ¨¡å‹é€²è¡Œé æ¸¬ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = make_pipeline(preprocessing, LinearRegression()) # å»ºç«‹ä¸€å€‹åŒ…å«æ•¸æ“šé è™•ç†çš„ Transformer å’Œç·šæ€§å›æ­¸æ¨¡å‹çš„ Pipeline\n",
    "linear_regression.fit(housing, housing_labels) # å°‡æ¨¡å‹æ“¬åˆåˆ°è¨“ç·´æ•¸æ“šä¸Š -> housing= åŒ…å«ç‰¹å¾µç‰¹å¾µçš„ DataFrame, housing_labels= ç›®æ¨™è®Šé‡ï¼ˆæˆ¿åƒ¹ä¸­ä½æ•¸ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ç·šæ€§å›æ­¸æ¨¡å‹å°å‰äº”å€‹æ¨£æœ¬çš„é æ¸¬çµæœç‚ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = linear_regression.predict(housing)\n",
    "housing_predictions[:5].round(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compare against the actual valuesï¼š\n",
    "\n",
    "- å¯ä»¥çœ‹åˆ°ï¼Œæ¨¡å‹çš„é æ¸¬èˆ‡å¯¦éš›å€¼æœ‰ä¸€äº›åå·®\n",
    "- é€šå¸¸æœƒä½¿ç”¨ä¸åŒçš„æŒ‡æ¨™ï¼ˆä¾‹å¦‚å‡æ–¹æ ¹èª¤å·®ï¼ŒRå¹³æ–¹ç­‰ï¼‰ä¾†è©•ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_labels.iloc[:5].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> è¨ˆç®—é æ¸¬å€¼å’Œå¯¦éš›å€¼ä¹‹é–“çš„èª¤å·®æ¯”ä¾‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¦‚æœèª¤å·®æ¯”ç‡ç‚º 0.1ï¼Œè¡¨ç¤ºé æ¸¬å€¼æ¯”å¯¦éš›å€¼å¤šäº† 10%ï¼›å¦‚æœèª¤å·®æ¯”ç‡ç‚º -0.1ï¼Œè¡¨ç¤ºé æ¸¬å€¼æ¯”å¯¦éš›å€¼å°‘äº† 10%ã€‚\n",
    "error_ratios = housing_predictions[:5].round(-2) / housing_labels.iloc[:5].values - 1 \n",
    "print(\", \".join([f\"{100 * ratio:.1f}%\" for ratio in error_ratios])) # join æ˜¯æŠŠå°‡å­—ç¬¦ä¸²åˆ—è¡¨ä¸­çš„å…ƒç´ ç”¨æŒ‡å®šçš„åˆ†éš”ç¬¦è™Ÿé€£æ¥èµ·ä¾†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> è¨ˆç®—ç·šæ€§å›æ­¸æ¨¡å‹çš„å‡æ–¹æ ¹èª¤å·®(RMSE)ï¼š\n",
    "\n",
    "- RMSE æ˜¯è¡¡é‡æ¨¡å‹é æ¸¬èª¤å·®çš„å¸¸è¦‹æŒ‡æ¨™\n",
    "- æ˜¯è¨ˆç®—é æ¸¬å€¼å’Œå¯¦éš›å€¼ä¹‹é–“çš„å¹³æ–¹èª¤å·®çš„å¹³å‡å€¼ï¼Œç„¶å¾Œå–å¹³æ–¹æ ¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin_rmse = mean_squared_error(housing_labels, housing_predictions, squared= False)\n",
    "lin_rmse "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Decision Tree Regressor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æƒ³åƒæœ‰ä¸€å †å•é¡Œï¼Œæ¯ä¸€å€‹å•é¡Œéƒ½æ˜¯åŸºæ–¼æŸå€‹ç‰¹å¾µçš„å€¼ã€‚é€šéå›ç­”é€™äº›å•é¡Œ â¡ï¸ æœ€çµ‚å¯ä»¥å¾—åˆ°ä¸€å€‹çµè«–æˆ–é æ¸¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_regression =  make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))\n",
    "tree_regression.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = tree_regression.predict(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_rmse = mean_squared_error(housing_labels, housing_predictions,\n",
    "                              squared=False)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ğŸš€ Better Evaluation Using Cross-Validation (äº¤å‰é©—è­‰)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– **Fine-Tune Your Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning æ¨¡å‹çš„ç›®çš„æ˜¯é€²ä¸€æ­¥èª¿æ•´ `å·²ç¶“è¨“ç·´é` çš„æ¨¡å‹ï¼Œè®“å®ƒæ›´å¥½åœ°å»é©æ‡‰ç‰¹å®šçš„ä»»å‹™æˆ–æ•¸æ“šã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ **Grid Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è¶…åƒæ•¸ (hyperparameters)**\n",
    "1. æ˜¯åœ¨è¨“ç·´æ¨¡å‹ä¹‹å‰éœ€è¦ã€Œæ‰‹å‹•èª¿æ•´ã€çš„åƒæ•¸ï¼Œå®ƒå€‘ä¸æœƒå¾è¨“ç·´æ•¸æ“šä¸­å­¸ç¿’ã€‚\n",
    "2. æ‰‹å‹•èª¿æ•´è¶…åƒæ•¸çš„ä¸€ç¨®æ–¹æ³•æ˜¯é€ä¸€å˜—è©¦ä¸åŒçš„å€¼ï¼Œç›´åˆ°æ‰¾åˆ°ã€Œæœ€ä½³çµ„åˆã€æœ€å¤§ç¨‹åº¦åœ°æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ç¶²æ ¼æœå°‹ (Grid Search)**ï¼šæ˜¯ä¸€ç¨®è‡ªå‹•åŒ–çš„è¶…åƒæ•¸èª¿æ•´æ–¹æ³•\n",
    "1. å®ƒé€šéã€ŒæŒ‡å®šã€è¶…åƒæ•¸çš„å¯èƒ½å–å€¼ç¯„åœï¼Œå°é€™äº›å€¼çš„ `æ‰€æœ‰çµ„åˆ` é€²è¡Œæœç´¢ã€‚\n",
    "2. ä½¿ç”¨äº¤å‰é©—è­‰ï¼ˆcross-validationï¼‰ä¾†è©•ä¼°æ¨¡å‹æ•ˆèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ **Randomized Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**éš¨æ©Ÿæœç´¢ï¼ˆRandomized Searchï¼‰**ï¼šæ˜¯ä¸€ç¨®è¶…åƒæ•¸èª¿æ•´çš„æ–¹æ³•\n",
    "1. åœ¨è¶…åƒæ•¸æœç´¢ç©ºé–“ä¸­éš¨æ©Ÿé¸æ“‡çµ„åˆé€²è¡Œè©•ä¼°ï¼Œè€Œä¸æ˜¯å˜—è©¦æ‰€æœ‰å¯èƒ½çš„çµ„åˆã€‚\n",
    "2. é©ç”¨æ–¼è¶…åƒæ•¸æœç´¢ç©ºé–“è¼ƒå¤§çš„æƒ…æ³\n",
    "3. ä¸éœ€è¦äº‹å…ˆæŒ‡å®šæ‰€æœ‰å¯èƒ½çš„è¶…åƒæ•¸çµ„åˆï¼Œè€Œæ˜¯ `æŒ‡å®šè¿­ä»£æ¬¡æ•¸`ã€‚\n",
    "4. åªè©•ä¼°å›ºå®šæ•¸é‡çš„éš¨æ©Ÿçµ„åˆï¼Œå®ƒé€šå¸¸æ¯”ç¶²æ ¼æœç´¢æ›´é«˜æ•ˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Grid Search verse Randomized Search\n",
    "1. å°æ–¼ `é€£çºŒè¶…åƒæ•¸`ï¼ˆæˆ–æ˜¯é›¢æ•£çš„ä½†å¯èƒ½å€¼å¾ˆå¤šçš„æƒ…æ³ï¼‰ï¼š\n",
    "\n",
    "    éš¨æ©Ÿæœç´¢åœ¨æ¯æ¬¡è¿­ä»£ä¸­éƒ½æœƒæ¢ç´¢ä¸åŒçš„å€¼ï¼Œè€Œä¸åƒ…åƒ…æ˜¯ç¶²æ ¼æœç´¢åˆ—å‡ºçš„é‚£å¹¾å€‹å€¼ã€‚\n",
    "2. æœ‰ `å¤šå€‹è¶…åƒæ•¸` éœ€è¦æ¢ç´¢æ™‚ï¼Œæ¯å€‹è¶…åƒæ•¸éƒ½æœ‰å¤šå€‹å¯èƒ½çš„å€¼ï¼š\n",
    "\n",
    "    **ç¶²æ ¼æœç´¢**å¯èƒ½éœ€è¦å¤§é‡è¨“ç·´æ¨¡å‹çš„æ¬¡æ•¸ï¼›**éš¨æ©Ÿæœç´¢**å¯ä»¥æ ¹æ“šé¸æ“‡çš„è¿­ä»£æ¬¡æ•¸é‹è¡Œ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ **Ensemble Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "çµåˆè¡¨ç¾æœ€å¥½çš„å¤šå€‹æ¨¡å‹ä¾†æå‡ç³»çµ±æ€§èƒ½çš„æ–¹æ³•ï¼Œé€™çµ„æ¨¡å‹é€šå¸¸æœƒ `æ¯”å–®å€‹æœ€ä½³æ¨¡å‹æ›´æœ‰æ•ˆ` ï¼Œç‰¹åˆ¥æ˜¯ç•¶å€‹åˆ¥æ¨¡å‹å‡ºç¾ä¸åŒé¡å‹éŒ¯èª¤çš„æƒ…æ³ä¸‹ã€‚\n",
    "\n",
    "1. éš¨æ©Ÿæ£®æ— Ramdom forests\n",
    "\n",
    "2. Train and Fine-tune a k-nearest -> create an ensemble model (CH7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ **Analyzing the Best Models and Their Errors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **RandomForestRegressor** can indicate the relative importance of each attribute for making accurate predictionsï¼š\n",
    "\n",
    "1. **ç²å–æœ€ä½³æ¨¡å‹**ï¼šå¾éš¨æ©Ÿæœç´¢ä¸­ç²å–æœ€ä½³æ¨¡å‹ï¼ŒåŒ…æ‹¬é è™•ç†æ­¥é©Ÿå’Œå¯¦éš›çš„ RandomForestRegressorã€‚\n",
    "\n",
    "2. **ç²å–ç‰¹å¾µé‡è¦æ€§**ï¼šä½¿ç”¨æœ€ä½³æ¨¡å‹ä¸­çš„ RandomForestRegressorï¼Œç²å–æ¯å€‹ç‰¹å¾µçš„ç›¸å°é‡è¦æ€§åˆ†æ•¸ã€‚\n",
    "\n",
    "3. **åˆ†æç‰¹å¾µé‡è¦æ€§**ï¼šå°‡ç‰¹å¾µé‡è¦æ€§åˆ†æ•¸æŒ‰é™åºæ’åˆ—ï¼Œé¡¯ç¤ºæ¯å€‹ç‰¹å¾µåç¨±å’Œå°æ‡‰çš„é‡è¦æ€§åˆ†æ•¸ã€‚\n",
    "\n",
    "4. **å„ªåŒ–æ¨¡å‹**ï¼šæ ¹æ“šç‰¹å¾µé‡è¦æ€§çš„çµæœï¼Œè€ƒæ…®æ˜¯å¦åˆªé™¤ä¸å¤ªæœ‰ç”¨çš„ç‰¹å¾µã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ **Evaluate Your System on the Test Set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "èª¿æ•´å®Œæ¨¡å‹å¾Œï¼Œæœ€å¾Œæœƒä½¿ç”¨ Testing set å°æœ€çµ‚æ¨¡å‹é€²è¡Œè©•ä¼°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ğŸš€ Launch, Monitor, and Maintain Your System**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data keeps evolving, you will **need to update your datasets and retrain your model** regularly. \n",
    "\n",
    "- å®šæœŸæ”¶é›†æ–°è³‡æ–™ä¸¦è¨­å®š Lable\n",
    "- è¨“ç·´æ¨¡å‹ä¸¦è‡ªå‹•å¾®èª¿è¶…åƒæ•¸ï¼Œæ ¹æ“šéœ€æ±‚è¨­å®šè‡ªå‹•åŸ·è¡Œé€±æœŸ\n",
    "- å¯«å¦ä¸€å€‹ç¨‹å¼ç¢¼ï¼Œåœ¨æ›´æ–°çš„æ¸¬è©¦é›†ä¸Šè©•ä¼°æ–°æ¨¡å‹å’Œä»¥å‰çš„æ¨¡å‹\n",
    "       \n",
    "    1. å¦‚æœæ•ˆèƒ½æœªä¸‹é™ï¼Œå‰‡å°‡æ¨¡å‹éƒ¨ç½²åˆ°ç”¢ç·šä¸­ã€‚\n",
    "    2. å¦‚æœæ•ˆèƒ½ç¢ºå¯¦ä¸‹é™ï¼Œç¢ºå¯¦åœ°èª¿æŸ¥ä¸‹é™åŸå› ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the modelâ€™s input data quality**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure you keep backups of every model**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
